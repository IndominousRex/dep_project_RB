{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random_state = 42\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dep_directory = \"..\\Identifying-depression-master\\Identifying-depression-master\\Data_Collector\\mixed_depression\"\n",
    "# non_dep_directory = \"..\\Identifying-depression-master\\Identifying-depression-master\\Data_Collector\\mixed_non_depression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_list_from_folder(directory, label: int):\n",
    "#     file_list = os.listdir(directory)\n",
    "\n",
    "#     data = []\n",
    "\n",
    "#     for file_name in file_list:\n",
    "#         file_path = os.path.join(directory, file_name)\n",
    "\n",
    "#         if os.path.isfile(file_path):\n",
    "#             with open(file_path, \"r\", encoding=\"ANSI\") as file:\n",
    "#                 text = file.read()\n",
    "#                 data.append(text)\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2822"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pickle.load(open(\"data/data.pkl\", \"rb\"))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplars = random.sample(data[:1300], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pickle.load(open(\"samples.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"#[A-Za-z0-9]*\", \"\", text)\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[\\n]\", \" \", text)\n",
    "    text = re.sub(r\"['\\\",*%$#()]\", \" \", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [[preprocess(i[0]), i[1]] for i in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"samples.pkl\",\"wb\")as f:\n",
    "#     pickle.dump(exemplars_preprocessed,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[preprocess(i[0]), i[1]] for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2822, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = open(\"samples.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186, '.*\\\\bdeath\\\\b\\\\s+(\\\\w+\\\\s+){0,3}\\\\brelief\\\\b.*\\n')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples[-6][0]), rules[-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This world just isn t for me I m scum. Bottom of the barrel. I m no role model and I don t try to become one. Death is scary but'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[-6][0][:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(\n",
    "    re.search(\n",
    "        r\"\\bdeath\\b\\s+(\\w+\\s+){0,8}\\brelief\\b\",\n",
    "        \"Death is scary but it s the only relief from the burden of having to change.\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(\n",
    "    r\"\\bdeath\\b\\s+(\\w+\\s+){0,8}\\brelief\\b\",\n",
    "    \"Death is scary but it s the only relief from the burden of having to change.\",\n",
    "    re.IGNORECASE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 39), match='Death is scary but it s the only relief'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "bool(\n",
    "    re.search(\n",
    "        r\".*\\b(dont|don't|no|not)\\s+(?:\\w+\\s+){0,3}to do with my life\\b.*\",\n",
    "        \"I dont have any idea what to do with my life.\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is it possible to fake depression? I have been feeling bad for about  months now. There are periods where I cry on a daily basis, I never feel like talking, I struggle concentrating and collecting thoughts, I feel such an extreme sadness inside of me also in happy situations, I am exhausted all the time even if I sleep  hours straight, I often have headaches, I am completely alone and I find it difficult to do anything. There are nights where its unbearable. I think about my situation: I am completely alone, I am failing at anything because I am so weak I cant handle my feelings, I feel guilt for everything that happens around me, I cant carry on anymore, I am always here in this dark place and I feel that I have no right to be like this. I used to self harm and I still have many scars on my skin, but I think to be disrespectful to those who truly struggle then I promised to cut never again. I shouldnt have the right to do so. My life is void, I am void, and I cant get the energy to do anything ever. I do not get pleasure from what I used to love, nothing seems to make me feel something good. I can enjoy no series, no book, no videogame like I used to. I think I dont even deserve to eat. I am just trying to pity myself giving myself depression, but I dont have it. Is it possible to fake depression? Could it actually be an excuse for me to not take responsibility of the failure that I am?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# sample_text = \"I feel like he's bored\"\n",
    "sample_text = \"i feel really depressing\"\n",
    "pattern = re.compile(r\".*\\bget|feel\\b\\s+(\\w+\\s+){0,3}\\bdepressed\\b.*\", re.IGNORECASE)\n",
    "\n",
    "bool(pattern.search(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor((1, 2, 3))\n",
    "b = torch.tensor((4, 5, 6))\n",
    "torch.cat((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8819444444444444"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x = pickle.load(\n",
    "    open(\"results/classification_2023-10-28_lr0.01_bs16_ed512_nhn512_results.pkl\", \"rb\")\n",
    ")\n",
    "\n",
    "max(x[\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_model_with_dropout0.5_1024nodes_1024embed_3e-4lr_32batch_results.pkl 0.8230279198399296\n",
      "rule_model_with_layernorm_1024nodes_1024embed_3e-4lr_32batch_results.pkl 0.8914872231306853\n",
      "rule_model_with_layernorm_1024nodes_1024embed_3e-4lr_32batch_rng42_results.pkl 0.8893321178577565\n",
      "\n",
      "\n",
      "classification_model_dropout0.5_2508_results.pkl 0.8962264150943396\n",
      "classification_model_dropout0.5_2608_with_embeddings_results.pkl 0.8310534594193945\n",
      "classification_model_dropout0.6_2508_results.pkl 0.878301887017376\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "dir = \"results\"\n",
    "list = os.listdir(dir)\n",
    "\n",
    "for i in list:\n",
    "    if \"rule\" in i:\n",
    "        print(i, pickle.load(open(f\"results/{i}\", \"rb\"))[\"test_acc\"][-1])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in list:\n",
    "    if \"classification\" in i:\n",
    "        print(i, pickle.load(open(f\"results/{i}\", \"rb\"))[\"test_acc\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Creating dataloaders...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from data_setup import get_dataloaders\n",
    "\n",
    "_, _, vocab_len = get_dataloaders(\"data.pkl\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22708"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utils import make_prediction\n",
    "from model import *\n",
    "\n",
    "rules = pickle.load(open(\"data/samples.pkl\", \"rb\"))\n",
    "data = pickle.load(open(\"data/data.pkl\", \"rb\"))\n",
    "\n",
    "classification_base_model = ClassificationNetwork(\n",
    "    vocab_len,\n",
    ")\n",
    "\n",
    "# for i in data:\n",
    "#     make_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the vocabulary...                              \n",
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!                                                 \n",
      "Creating dataloaders...                               \n",
      "Done!                                                 \n",
      "  0%|          | 0/10 [00:02<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7388660557364ed48b62ccf4b2d5dc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "Epoch: 1 | train_loss: 195.3203 | train_acc: 0.7798 | train_prec: 0.7991 | train_recall: 0.9687 | train_f1: 0.8736 | test_loss: 125.3476 | test_acc: 0.7810 | test_prec: 0.7810 | test_recall: 1.0000 | test_f1: 0.8757 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 2 | train_loss: 127.8076 | train_acc: 0.7997 | train_prec: 0.7997 | train_recall: 1.0000 | train_f1: 0.8878 | test_loss: 124.1852 | test_acc: 0.7792 | test_prec: 0.7792 | test_recall: 1.0000 | test_f1: 0.8752 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 3 | train_loss: 126.9740 | train_acc: 0.8004 | train_prec: 0.8004 | train_recall: 1.0000 | train_f1: 0.8881 | test_loss: 124.2612 | test_acc: 0.7795 | test_prec: 0.7795 | test_recall: 1.0000 | test_f1: 0.8748 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 4 | train_loss: 126.1113 | train_acc: 0.8001 | train_prec: 0.8001 | train_recall: 1.0000 | train_f1: 0.8883 | test_loss: 124.4867 | test_acc: 0.7827 | test_prec: 0.7827 | test_recall: 1.0000 | test_f1: 0.8772 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 5 | train_loss: 125.5447 | train_acc: 0.8002 | train_prec: 0.8002 | train_recall: 1.0000 | train_f1: 0.8882 | test_loss: 125.2987 | test_acc: 0.7815 | test_prec: 0.7812 | test_recall: 1.0000 | test_f1: 0.8765 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 6 | train_loss: 125.3389 | train_acc: 0.8008 | train_prec: 0.8007 | train_recall: 1.0000 | train_f1: 0.8883 | test_loss: 125.3677 | test_acc: 0.7852 | test_prec: 0.7846 | test_recall: 1.0000 | test_f1: 0.8786 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 7 | train_loss: 125.0714 | train_acc: 0.8021 | train_prec: 0.8017 | train_recall: 1.0000 | train_f1: 0.8892 | test_loss: 125.3023 | test_acc: 0.7847 | test_prec: 0.7837 | test_recall: 1.0000 | test_f1: 0.8780 | \n",
      "\n",
      "Epoch 00008: reducing learning rate of group 0 to 7.6014e-04.\n",
      "                                                      \n",
      "Epoch: 8 | train_loss: 124.7362 | train_acc: 0.8041 | train_prec: 0.8033 | train_recall: 1.0000 | train_f1: 0.8901 | test_loss: 125.8637 | test_acc: 0.7860 | test_prec: 0.7847 | test_recall: 0.9999 | test_f1: 0.8780 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 9 | train_loss: 124.2524 | train_acc: 0.8068 | train_prec: 0.8055 | train_recall: 1.0000 | train_f1: 0.8912 | test_loss: 125.5600 | test_acc: 0.7847 | test_prec: 0.7834 | test_recall: 0.9996 | test_f1: 0.8774 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 10 | train_loss: 124.0030 | train_acc: 0.8072 | train_prec: 0.8056 | train_recall: 1.0000 | train_f1: 0.8913 | test_loss: 126.1418 | test_acc: 0.7879 | test_prec: 0.7865 | test_recall: 0.9997 | test_f1: 0.8795 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 11 | train_loss: 124.0000 | train_acc: 0.8088 | train_prec: 0.8070 | train_recall: 1.0000 | train_f1: 0.8923 | test_loss: 126.2851 | test_acc: 0.7875 | test_prec: 0.7863 | test_recall: 0.9996 | test_f1: 0.8793 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 12 | train_loss: 123.9005 | train_acc: 0.8100 | train_prec: 0.8079 | train_recall: 1.0000 | train_f1: 0.8929 | test_loss: 126.9547 | test_acc: 0.7903 | test_prec: 0.7887 | test_recall: 0.9994 | test_f1: 0.8806 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 13 | train_loss: 123.9590 | train_acc: 0.8122 | train_prec: 0.8098 | train_recall: 1.0000 | train_f1: 0.8941 | test_loss: 127.1099 | test_acc: 0.7900 | test_prec: 0.7886 | test_recall: 0.9994 | test_f1: 0.8804 | \n",
      "\n",
      "Epoch 00014: reducing learning rate of group 0 to 7.6014e-05.\n",
      "                                                      \n",
      "Epoch: 14 | train_loss: 123.8519 | train_acc: 0.8137 | train_prec: 0.8110 | train_recall: 1.0000 | train_f1: 0.8949 | test_loss: 126.9962 | test_acc: 0.7899 | test_prec: 0.7883 | test_recall: 0.9993 | test_f1: 0.8804 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 15 | train_loss: 123.8654 | train_acc: 0.8147 | train_prec: 0.8118 | train_recall: 1.0000 | train_f1: 0.8951 | test_loss: 127.5715 | test_acc: 0.7926 | test_prec: 0.7909 | test_recall: 0.9993 | test_f1: 0.8818 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 16 | train_loss: 123.8765 | train_acc: 0.8152 | train_prec: 0.8123 | train_recall: 1.0000 | train_f1: 0.8956 | test_loss: 126.7524 | test_acc: 0.7881 | test_prec: 0.7864 | test_recall: 0.9993 | test_f1: 0.8788 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 17 | train_loss: 123.6854 | train_acc: 0.8148 | train_prec: 0.8117 | train_recall: 1.0000 | train_f1: 0.8952 | test_loss: 126.7609 | test_acc: 0.7891 | test_prec: 0.7870 | test_recall: 0.9993 | test_f1: 0.8797 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 18 | train_loss: 123.7390 | train_acc: 0.8151 | train_prec: 0.8121 | train_recall: 1.0000 | train_f1: 0.8956 | test_loss: 127.5238 | test_acc: 0.7921 | test_prec: 0.7903 | test_recall: 0.9993 | test_f1: 0.8818 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 19 | train_loss: 123.7521 | train_acc: 0.8155 | train_prec: 0.8125 | train_recall: 1.0000 | train_f1: 0.8958 | test_loss: 127.2721 | test_acc: 0.7908 | test_prec: 0.7889 | test_recall: 0.9993 | test_f1: 0.8811 | \n",
      "\n",
      "Epoch 00020: reducing learning rate of group 0 to 7.6014e-06.\n",
      "                                                      \n",
      "Epoch: 20 | train_loss: 123.7112 | train_acc: 0.8155 | train_prec: 0.8124 | train_recall: 1.0000 | train_f1: 0.8953 | test_loss: 127.5309 | test_acc: 0.7916 | test_prec: 0.7896 | test_recall: 0.9993 | test_f1: 0.8814 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 21 | train_loss: 123.7976 | train_acc: 0.8162 | train_prec: 0.8130 | train_recall: 1.0000 | train_f1: 0.8960 | test_loss: 127.0505 | test_acc: 0.7894 | test_prec: 0.7875 | test_recall: 0.9993 | test_f1: 0.8798 | \n",
      "\n",
      "                                                      \n",
      "Epoch: 22 | train_loss: 123.8107 | train_acc: 0.8162 | train_prec: 0.8130 | train_recall: 1.0000 | train_f1: 0.8960 | test_loss: 127.6997 | test_acc: 0.7936 | test_prec: 0.7917 | test_recall: 0.9993 | test_f1: 0.8825 | \n",
      "\n",
      "                                                      \n",
      "Early stopped training at epoch: 22\n",
      "Best test loss : 124.1852 at epoch: 1                 \n",
      "Making the vocabulary...                                                        \n",
      "Done!                                                                           \n",
      "Creating dataloaders...                                                         \n",
      "Done!                                                                           \n",
      " 10%|█         | 1/10 [01:27<13:05, 87.26s/trial, best loss: 124.18518744574652]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c9a26700f3409d87989deff16edf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "Epoch: 1 | train_loss: 136.4198 | train_acc: 0.7875 | train_prec: 0.7946 | train_recall: 0.9879 | train_f1: 0.8782 | test_loss: 127.2211 | test_acc: 0.7923 | test_prec: 0.7923 | test_recall: 1.0000 | test_f1: 0.8822 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 2 | train_loss: 126.8601 | train_acc: 0.7949 | train_prec: 0.7948 | train_recall: 0.9998 | train_f1: 0.8838 | test_loss: 126.2436 | test_acc: 0.7939 | test_prec: 0.7937 | test_recall: 0.9998 | test_f1: 0.8826 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 3 | train_loss: 125.9453 | train_acc: 0.8110 | train_prec: 0.8078 | train_recall: 0.9999 | train_f1: 0.8919 | test_loss: 126.2425 | test_acc: 0.8131 | test_prec: 0.8101 | test_recall: 0.9983 | test_f1: 0.8922 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 4 | train_loss: 125.1722 | train_acc: 0.8513 | train_prec: 0.8416 | train_recall: 0.9999 | train_f1: 0.9125 | test_loss: 126.0468 | test_acc: 0.8413 | test_prec: 0.8352 | test_recall: 0.9958 | test_f1: 0.9069 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 5 | train_loss: 124.5783 | train_acc: 0.8794 | train_prec: 0.8670 | train_recall: 0.9998 | train_f1: 0.9276 | test_loss: 126.5124 | test_acc: 0.8630 | test_prec: 0.8563 | test_recall: 0.9925 | test_f1: 0.9181 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 6 | train_loss: 124.2289 | train_acc: 0.8890 | train_prec: 0.8757 | train_recall: 0.9999 | train_f1: 0.9328 | test_loss: 126.7325 | test_acc: 0.8655 | test_prec: 0.8611 | test_recall: 0.9892 | test_f1: 0.9197 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 7 | train_loss: 123.9511 | train_acc: 0.8972 | train_prec: 0.8837 | train_recall: 0.9999 | train_f1: 0.9374 | test_loss: 127.3231 | test_acc: 0.8720 | test_prec: 0.8714 | test_recall: 0.9819 | test_f1: 0.9223 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 8 | train_loss: 123.6712 | train_acc: 0.9081 | train_prec: 0.8945 | train_recall: 0.9999 | train_f1: 0.9436 | test_loss: 127.8013 | test_acc: 0.8780 | test_prec: 0.8804 | test_recall: 0.9770 | test_f1: 0.9246 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 9 | train_loss: 123.4131 | train_acc: 0.9185 | train_prec: 0.9052 | train_recall: 0.9999 | train_f1: 0.9497 | test_loss: 127.3433 | test_acc: 0.8764 | test_prec: 0.8754 | test_recall: 0.9824 | test_f1: 0.9242 | \n",
      "\n",
      "Epoch 00010: reducing learning rate of group 0 to 5.5377e-04.                   \n",
      "                                                                                \n",
      "Epoch: 10 | train_loss: 123.1997 | train_acc: 0.9291 | train_prec: 0.9164 | train_recall: 0.9999 | train_f1: 0.9559 | test_loss: 127.9568 | test_acc: 0.8877 | test_prec: 0.8926 | test_recall: 0.9741 | test_f1: 0.9307 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 11 | train_loss: 122.8534 | train_acc: 0.9344 | train_prec: 0.9220 | train_recall: 0.9999 | train_f1: 0.9591 | test_loss: 128.6617 | test_acc: 0.8877 | test_prec: 0.8969 | test_recall: 0.9696 | test_f1: 0.9310 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 12 | train_loss: 122.7485 | train_acc: 0.9343 | train_prec: 0.9214 | train_recall: 1.0000 | train_f1: 0.9586 | test_loss: 128.8331 | test_acc: 0.8876 | test_prec: 0.8953 | test_recall: 0.9705 | test_f1: 0.9306 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 13 | train_loss: 122.6620 | train_acc: 0.9352 | train_prec: 0.9226 | train_recall: 1.0000 | train_f1: 0.9593 | test_loss: 129.0552 | test_acc: 0.8883 | test_prec: 0.8952 | test_recall: 0.9718 | test_f1: 0.9307 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 14 | train_loss: 122.6132 | train_acc: 0.9361 | train_prec: 0.9237 | train_recall: 1.0000 | train_f1: 0.9600 | test_loss: 129.3219 | test_acc: 0.8883 | test_prec: 0.8951 | test_recall: 0.9718 | test_f1: 0.9308 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 15 | train_loss: 122.5468 | train_acc: 0.9369 | train_prec: 0.9245 | train_recall: 1.0000 | train_f1: 0.9604 | test_loss: 129.4856 | test_acc: 0.8875 | test_prec: 0.8937 | test_recall: 0.9720 | test_f1: 0.9303 | \n",
      "\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.5377e-05.                   \n",
      "                                                                                \n",
      "Epoch: 16 | train_loss: 122.5058 | train_acc: 0.9379 | train_prec: 0.9257 | train_recall: 1.0000 | train_f1: 0.9611 | test_loss: 129.8178 | test_acc: 0.8888 | test_prec: 0.8957 | test_recall: 0.9713 | test_f1: 0.9311 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 17 | train_loss: 122.4281 | train_acc: 0.9387 | train_prec: 0.9267 | train_recall: 1.0000 | train_f1: 0.9616 | test_loss: 129.8745 | test_acc: 0.8882 | test_prec: 0.8945 | test_recall: 0.9715 | test_f1: 0.9304 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 18 | train_loss: 122.4170 | train_acc: 0.9386 | train_prec: 0.9264 | train_recall: 1.0000 | train_f1: 0.9614 | test_loss: 129.9800 | test_acc: 0.8883 | test_prec: 0.8952 | test_recall: 0.9717 | test_f1: 0.9308 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 19 | train_loss: 122.4132 | train_acc: 0.9387 | train_prec: 0.9264 | train_recall: 1.0000 | train_f1: 0.9614 | test_loss: 130.0544 | test_acc: 0.8885 | test_prec: 0.8954 | test_recall: 0.9715 | test_f1: 0.9310 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 20 | train_loss: 122.4178 | train_acc: 0.9389 | train_prec: 0.9272 | train_recall: 1.0000 | train_f1: 0.9619 | test_loss: 130.0621 | test_acc: 0.8887 | test_prec: 0.8956 | test_recall: 0.9728 | test_f1: 0.9318 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 21 | train_loss: 122.3995 | train_acc: 0.9390 | train_prec: 0.9268 | train_recall: 1.0000 | train_f1: 0.9617 | test_loss: 130.1163 | test_acc: 0.8883 | test_prec: 0.8947 | test_recall: 0.9720 | test_f1: 0.9308 | \n",
      "\n",
      "Epoch 00022: reducing learning rate of group 0 to 5.5377e-06.                   \n",
      "                                                                                \n",
      "Epoch: 22 | train_loss: 122.3982 | train_acc: 0.9392 | train_prec: 0.9274 | train_recall: 1.0000 | train_f1: 0.9620 | test_loss: 130.1972 | test_acc: 0.8885 | test_prec: 0.8954 | test_recall: 0.9722 | test_f1: 0.9314 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 23 | train_loss: 122.3792 | train_acc: 0.9393 | train_prec: 0.9273 | train_recall: 1.0000 | train_f1: 0.9619 | test_loss: 130.2017 | test_acc: 0.8885 | test_prec: 0.8953 | test_recall: 0.9730 | test_f1: 0.9314 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 24 | train_loss: 122.3739 | train_acc: 0.9393 | train_prec: 0.9265 | train_recall: 1.0000 | train_f1: 0.9613 | test_loss: 130.2090 | test_acc: 0.8882 | test_prec: 0.8944 | test_recall: 0.9717 | test_f1: 0.9304 | \n",
      "\n",
      "                                                                                \n",
      "Early stopped training at epoch: 24\n",
      "Best test loss : 126.0468 at epoch: 3                                           \n",
      "Making the vocabulary...                                                        \n",
      "Done!                                                                           \n",
      "Creating dataloaders...                                                         \n",
      "Done!                                                                           \n",
      " 20%|██        | 2/10 [02:59<12:03, 90.41s/trial, best loss: 124.18518744574652]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ee49b2c24b45a0a13448f765d8391a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "Epoch: 1 | train_loss: 311.6672 | train_acc: 0.6659 | train_prec: 0.7914 | train_recall: 0.7886 | train_f1: 0.7861 | test_loss: 204.2032 | test_acc: 0.7409 | test_prec: 0.7922 | test_recall: 0.9113 | test_f1: 0.8475 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 2 | train_loss: 159.6418 | train_acc: 0.7658 | train_prec: 0.8029 | train_recall: 0.9373 | train_f1: 0.8648 | test_loss: 139.5333 | test_acc: 0.7614 | test_prec: 0.7874 | test_recall: 0.9531 | test_f1: 0.8623 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 3 | train_loss: 135.4497 | train_acc: 0.7978 | train_prec: 0.8042 | train_recall: 0.9881 | train_f1: 0.8866 | test_loss: 130.8324 | test_acc: 0.7907 | test_prec: 0.7917 | test_recall: 0.9971 | test_f1: 0.8825 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 4 | train_loss: 130.5478 | train_acc: 0.7998 | train_prec: 0.8004 | train_recall: 0.9983 | train_f1: 0.8883 | test_loss: 126.9229 | test_acc: 0.7824 | test_prec: 0.7824 | test_recall: 0.9994 | test_f1: 0.8776 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 5 | train_loss: 129.5548 | train_acc: 0.8014 | train_prec: 0.8015 | train_recall: 0.9994 | train_f1: 0.8895 | test_loss: 127.1034 | test_acc: 0.7868 | test_prec: 0.7865 | test_recall: 0.9999 | test_f1: 0.8804 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 6 | train_loss: 129.1280 | train_acc: 0.8021 | train_prec: 0.8020 | train_recall: 0.9998 | train_f1: 0.8899 | test_loss: 127.8748 | test_acc: 0.7925 | test_prec: 0.7922 | test_recall: 0.9998 | test_f1: 0.8839 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 7 | train_loss: 128.5717 | train_acc: 0.8011 | train_prec: 0.8009 | train_recall: 0.9998 | train_f1: 0.8892 | test_loss: 127.0231 | test_acc: 0.7896 | test_prec: 0.7893 | test_recall: 0.9999 | test_f1: 0.8821 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 8 | train_loss: 128.3629 | train_acc: 0.8014 | train_prec: 0.8012 | train_recall: 0.9999 | train_f1: 0.8895 | test_loss: 128.2059 | test_acc: 0.7967 | test_prec: 0.7965 | test_recall: 0.9999 | test_f1: 0.8864 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 9 | train_loss: 128.0987 | train_acc: 0.8011 | train_prec: 0.8008 | train_recall: 0.9999 | train_f1: 0.8892 | test_loss: 125.9221 | test_acc: 0.7859 | test_prec: 0.7856 | test_recall: 0.9997 | test_f1: 0.8796 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 10 | train_loss: 128.3764 | train_acc: 0.8036 | train_prec: 0.8033 | train_recall: 0.9999 | train_f1: 0.8908 | test_loss: 125.5940 | test_acc: 0.7839 | test_prec: 0.7835 | test_recall: 0.9999 | test_f1: 0.8785 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 11 | train_loss: 127.9940 | train_acc: 0.8028 | train_prec: 0.8024 | train_recall: 0.9999 | train_f1: 0.8903 | test_loss: 126.6708 | test_acc: 0.7915 | test_prec: 0.7911 | test_recall: 0.9999 | test_f1: 0.8833 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 12 | train_loss: 127.8120 | train_acc: 0.8030 | train_prec: 0.8026 | train_recall: 0.9999 | train_f1: 0.8904 | test_loss: 127.0817 | test_acc: 0.7959 | test_prec: 0.7953 | test_recall: 0.9996 | test_f1: 0.8857 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 13 | train_loss: 127.3893 | train_acc: 0.8017 | train_prec: 0.8012 | train_recall: 0.9999 | train_f1: 0.8894 | test_loss: 127.8234 | test_acc: 0.7991 | test_prec: 0.7986 | test_recall: 0.9996 | test_f1: 0.8875 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 14 | train_loss: 127.3475 | train_acc: 0.8024 | train_prec: 0.8018 | train_recall: 0.9999 | train_f1: 0.8898 | test_loss: 126.7558 | test_acc: 0.7935 | test_prec: 0.7930 | test_recall: 0.9996 | test_f1: 0.8842 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 15 | train_loss: 127.4573 | train_acc: 0.8047 | train_prec: 0.8039 | train_recall: 0.9999 | train_f1: 0.8911 | test_loss: 125.7029 | test_acc: 0.7895 | test_prec: 0.7887 | test_recall: 0.9996 | test_f1: 0.8816 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 16 | train_loss: 127.3098 | train_acc: 0.8059 | train_prec: 0.8048 | train_recall: 0.9999 | train_f1: 0.8917 | test_loss: 125.2602 | test_acc: 0.7885 | test_prec: 0.7874 | test_recall: 0.9997 | test_f1: 0.8809 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 17 | train_loss: 127.6206 | train_acc: 0.8089 | train_prec: 0.8077 | train_recall: 0.9999 | train_f1: 0.8934 | test_loss: 124.4373 | test_acc: 0.7838 | test_prec: 0.7827 | test_recall: 0.9995 | test_f1: 0.8776 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 18 | train_loss: 127.2434 | train_acc: 0.8094 | train_prec: 0.8079 | train_recall: 0.9999 | train_f1: 0.8936 | test_loss: 126.0677 | test_acc: 0.7924 | test_prec: 0.7914 | test_recall: 0.9996 | test_f1: 0.8834 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 19 | train_loss: 127.0167 | train_acc: 0.8110 | train_prec: 0.8090 | train_recall: 0.9999 | train_f1: 0.8943 | test_loss: 128.0134 | test_acc: 0.8064 | test_prec: 0.8052 | test_recall: 0.9995 | test_f1: 0.8914 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 20 | train_loss: 126.9469 | train_acc: 0.8130 | train_prec: 0.8108 | train_recall: 0.9998 | train_f1: 0.8953 | test_loss: 125.9631 | test_acc: 0.8012 | test_prec: 0.7989 | test_recall: 0.9994 | test_f1: 0.8879 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 21 | train_loss: 127.0068 | train_acc: 0.8170 | train_prec: 0.8142 | train_recall: 0.9999 | train_f1: 0.8974 | test_loss: 124.9131 | test_acc: 0.7954 | test_prec: 0.7930 | test_recall: 0.9994 | test_f1: 0.8842 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 22 | train_loss: 126.8596 | train_acc: 0.8189 | train_prec: 0.8158 | train_recall: 0.9999 | train_f1: 0.8984 | test_loss: 123.5415 | test_acc: 0.7861 | test_prec: 0.7841 | test_recall: 0.9994 | test_f1: 0.8783 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 23 | train_loss: 126.5629 | train_acc: 0.8200 | train_prec: 0.8165 | train_recall: 0.9999 | train_f1: 0.8988 | test_loss: 125.3234 | test_acc: 0.8038 | test_prec: 0.8002 | test_recall: 0.9993 | test_f1: 0.8887 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 24 | train_loss: 126.3111 | train_acc: 0.8246 | train_prec: 0.8201 | train_recall: 0.9999 | train_f1: 0.9011 | test_loss: 124.8385 | test_acc: 0.8045 | test_prec: 0.8005 | test_recall: 0.9990 | test_f1: 0.8886 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 25 | train_loss: 126.6186 | train_acc: 0.8323 | train_prec: 0.8270 | train_recall: 0.9999 | train_f1: 0.9052 | test_loss: 123.6664 | test_acc: 0.7942 | test_prec: 0.7907 | test_recall: 0.9992 | test_f1: 0.8824 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 26 | train_loss: 126.1881 | train_acc: 0.8343 | train_prec: 0.8284 | train_recall: 0.9999 | train_f1: 0.9060 | test_loss: 124.8337 | test_acc: 0.8091 | test_prec: 0.8041 | test_recall: 0.9986 | test_f1: 0.8904 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 27 | train_loss: 126.2207 | train_acc: 0.8422 | train_prec: 0.8354 | train_recall: 0.9999 | train_f1: 0.9103 | test_loss: 126.9003 | test_acc: 0.8185 | test_prec: 0.8145 | test_recall: 0.9984 | test_f1: 0.8969 | \n",
      "\n",
      "Epoch 00028: reducing learning rate of group 0 to 4.3163e-04.                   \n",
      "                                                                                \n",
      "Epoch: 28 | train_loss: 125.6295 | train_acc: 0.8453 | train_prec: 0.8376 | train_recall: 1.0000 | train_f1: 0.9115 | test_loss: 124.9703 | test_acc: 0.8117 | test_prec: 0.8068 | test_recall: 0.9985 | test_f1: 0.8924 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 29 | train_loss: 125.7574 | train_acc: 0.8476 | train_prec: 0.8400 | train_recall: 1.0000 | train_f1: 0.9130 | test_loss: 124.5927 | test_acc: 0.8117 | test_prec: 0.8067 | test_recall: 0.9976 | test_f1: 0.8920 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 30 | train_loss: 125.9820 | train_acc: 0.8526 | train_prec: 0.8446 | train_recall: 1.0000 | train_f1: 0.9156 | test_loss: 124.6064 | test_acc: 0.8124 | test_prec: 0.8072 | test_recall: 0.9981 | test_f1: 0.8925 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 31 | train_loss: 125.9654 | train_acc: 0.8524 | train_prec: 0.8443 | train_recall: 1.0000 | train_f1: 0.9155 | test_loss: 124.1812 | test_acc: 0.8139 | test_prec: 0.8079 | test_recall: 0.9986 | test_f1: 0.8930 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 32 | train_loss: 125.9227 | train_acc: 0.8514 | train_prec: 0.8435 | train_recall: 1.0000 | train_f1: 0.9150 | test_loss: 126.7249 | test_acc: 0.8250 | test_prec: 0.8202 | test_recall: 0.9980 | test_f1: 0.9003 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 33 | train_loss: 125.7232 | train_acc: 0.8520 | train_prec: 0.8439 | train_recall: 1.0000 | train_f1: 0.9152 | test_loss: 126.0403 | test_acc: 0.8202 | test_prec: 0.8150 | test_recall: 0.9984 | test_f1: 0.8974 | \n",
      "\n",
      "Epoch 00034: reducing learning rate of group 0 to 4.3163e-05.                   \n",
      "                                                                                \n",
      "Epoch: 34 | train_loss: 125.7285 | train_acc: 0.8528 | train_prec: 0.8446 | train_recall: 1.0000 | train_f1: 0.9157 | test_loss: 127.3931 | test_acc: 0.8246 | test_prec: 0.8204 | test_recall: 0.9977 | test_f1: 0.9003 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 35 | train_loss: 125.5725 | train_acc: 0.8530 | train_prec: 0.8446 | train_recall: 1.0000 | train_f1: 0.9157 | test_loss: 126.4889 | test_acc: 0.8216 | test_prec: 0.8171 | test_recall: 0.9979 | test_f1: 0.8984 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 36 | train_loss: 125.8058 | train_acc: 0.8543 | train_prec: 0.8460 | train_recall: 0.9999 | train_f1: 0.9165 | test_loss: 125.9685 | test_acc: 0.8205 | test_prec: 0.8154 | test_recall: 0.9984 | test_f1: 0.8976 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 37 | train_loss: 125.7500 | train_acc: 0.8533 | train_prec: 0.8450 | train_recall: 1.0000 | train_f1: 0.9158 | test_loss: 126.8293 | test_acc: 0.8232 | test_prec: 0.8185 | test_recall: 0.9981 | test_f1: 0.8994 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 38 | train_loss: 125.6906 | train_acc: 0.8531 | train_prec: 0.8448 | train_recall: 1.0000 | train_f1: 0.9158 | test_loss: 126.1581 | test_acc: 0.8190 | test_prec: 0.8146 | test_recall: 0.9975 | test_f1: 0.8968 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 39 | train_loss: 126.0452 | train_acc: 0.8548 | train_prec: 0.8467 | train_recall: 1.0000 | train_f1: 0.9169 | test_loss: 125.4026 | test_acc: 0.8181 | test_prec: 0.8128 | test_recall: 0.9984 | test_f1: 0.8961 | \n",
      "\n",
      "Epoch 00040: reducing learning rate of group 0 to 4.3163e-06.                   \n",
      "                                                                                \n",
      "Epoch: 40 | train_loss: 125.9726 | train_acc: 0.8549 | train_prec: 0.8467 | train_recall: 1.0000 | train_f1: 0.9169 | test_loss: 124.8616 | test_acc: 0.8176 | test_prec: 0.8118 | test_recall: 0.9984 | test_f1: 0.8954 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 41 | train_loss: 125.9512 | train_acc: 0.8543 | train_prec: 0.8461 | train_recall: 1.0000 | train_f1: 0.9166 | test_loss: 125.5353 | test_acc: 0.8173 | test_prec: 0.8129 | test_recall: 0.9967 | test_f1: 0.8955 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 42 | train_loss: 125.8775 | train_acc: 0.8543 | train_prec: 0.8461 | train_recall: 1.0000 | train_f1: 0.9165 | test_loss: 125.6750 | test_acc: 0.8183 | test_prec: 0.8134 | test_recall: 0.9979 | test_f1: 0.8962 | \n",
      "\n",
      "                                                                                \n",
      "Early stopped training at epoch: 42\n",
      "Best test loss : 123.5415 at epoch: 21                                          \n",
      "Making the vocabulary...                                                        \n",
      "Done!                                                                           \n",
      "Creating dataloaders...                                                         \n",
      "Done!                                                                           \n",
      " 30%|███       | 3/10 [04:06<09:15, 79.39s/trial, best loss: 123.54146766662598]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff9ebfe695045439dce33885b889ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \n",
      "Epoch: 1 | train_loss: 189.0125 | train_acc: 0.5104 | train_prec: 0.8422 | train_recall: 0.4736 | train_f1: 0.6055 | test_loss: 151.5497 | test_acc: 0.5113 | test_prec: 0.8595 | test_recall: 0.4615 | test_f1: 0.5999 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 2 | train_loss: 142.6757 | train_acc: 0.5065 | train_prec: 0.8715 | train_recall: 0.4455 | train_f1: 0.5890 | test_loss: 140.3334 | test_acc: 0.5107 | test_prec: 0.8861 | test_recall: 0.4419 | test_f1: 0.5896 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 3 | train_loss: 135.8018 | train_acc: 0.5134 | train_prec: 0.8957 | train_recall: 0.4398 | train_f1: 0.5894 | test_loss: 135.7901 | test_acc: 0.5015 | test_prec: 0.8890 | test_recall: 0.4269 | test_f1: 0.5765 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 4 | train_loss: 132.7925 | train_acc: 0.5176 | train_prec: 0.9111 | train_recall: 0.4367 | train_f1: 0.5897 | test_loss: 133.1573 | test_acc: 0.5151 | test_prec: 0.9057 | test_recall: 0.4372 | test_f1: 0.5884 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 5 | train_loss: 131.0904 | train_acc: 0.5264 | train_prec: 0.9217 | train_recall: 0.4427 | train_f1: 0.5975 | test_loss: 131.3926 | test_acc: 0.5352 | test_prec: 0.9209 | test_recall: 0.4525 | test_f1: 0.6064 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 6 | train_loss: 129.9601 | train_acc: 0.5359 | train_prec: 0.9319 | train_recall: 0.4500 | train_f1: 0.6061 | test_loss: 130.0766 | test_acc: 0.5487 | test_prec: 0.9279 | test_recall: 0.4661 | test_f1: 0.6199 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 7 | train_loss: 129.3209 | train_acc: 0.5493 | train_prec: 0.9381 | train_recall: 0.4646 | train_f1: 0.6205 | test_loss: 128.2618 | test_acc: 0.5482 | test_prec: 0.9261 | test_recall: 0.4616 | test_f1: 0.6155 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 8 | train_loss: 128.9021 | train_acc: 0.5562 | train_prec: 0.9436 | train_recall: 0.4717 | train_f1: 0.6279 | test_loss: 129.2162 | test_acc: 0.5371 | test_prec: 0.9262 | test_recall: 0.4522 | test_f1: 0.6071 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 9 | train_loss: 128.3849 | train_acc: 0.5678 | train_prec: 0.9458 | train_recall: 0.4850 | train_f1: 0.6403 | test_loss: 128.0527 | test_acc: 0.5742 | test_prec: 0.9350 | test_recall: 0.4956 | test_f1: 0.6467 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 10 | train_loss: 128.1278 | train_acc: 0.5797 | train_prec: 0.9493 | train_recall: 0.5000 | train_f1: 0.6537 | test_loss: 129.1965 | test_acc: 0.5571 | test_prec: 0.9276 | test_recall: 0.4810 | test_f1: 0.6331 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 11 | train_loss: 127.8658 | train_acc: 0.5862 | train_prec: 0.9503 | train_recall: 0.5076 | train_f1: 0.6606 | test_loss: 129.2327 | test_acc: 0.5702 | test_prec: 0.9383 | test_recall: 0.4939 | test_f1: 0.6467 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 12 | train_loss: 127.7088 | train_acc: 0.5934 | train_prec: 0.9527 | train_recall: 0.5154 | train_f1: 0.6682 | test_loss: 128.3000 | test_acc: 0.5802 | test_prec: 0.9396 | test_recall: 0.5049 | test_f1: 0.6559 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 13 | train_loss: 127.4543 | train_acc: 0.6001 | train_prec: 0.9533 | train_recall: 0.5242 | train_f1: 0.6754 | test_loss: 129.2365 | test_acc: 0.5778 | test_prec: 0.9433 | test_recall: 0.5053 | test_f1: 0.6566 | \n",
      "\n",
      "                                                                                \n",
      "Epoch: 14 | train_loss: 127.2824 | train_acc: 0.6078 | train_prec: 0.9544 | train_recall: 0.5336 | train_f1: 0.6835 | test_loss: 128.8668 | test_acc: 0.5964 | test_prec: 0.9394 | test_recall: 0.5298 | test_f1: 0.6768 | \n",
      "\n",
      " 30%|███       | 3/10 [04:47<11:10, 95.73s/trial, best loss: 123.54146766662598]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aritr\\OneDrive\\Documents\\Coding\\Python\\NLP\\dep_project_RB\\dummy.ipynb Cell 29\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m space \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: hp\u001b[39m.\u001b[39muniform(\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0.0001\u001b[39m, \u001b[39m0.01\u001b[39m),  \u001b[39m# Learning rate\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: hp\u001b[39m.\u001b[39mchoice(\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39m4\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m256\u001b[39m]),  \u001b[39m# Batch size\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     ),  \u001b[39m# Number of nodes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Run the optimization\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m best \u001b[39m=\u001b[39m fmin(fn\u001b[39m=\u001b[39;49mobjective, space\u001b[39m=\u001b[39;49mspace, algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest, max_evals\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Retrieve the best hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Hyperparameters:\u001b[39m\u001b[39m\"\u001b[39m, best)\n",
      "File \u001b[1;32mc:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[0;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[0;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[0;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[0;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[0;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[1;32mc:\\Users\\aritr\\OneDrive\\Documents\\Coding\\Python\\NLP\\dep_project_RB\\dummy.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m scheduler \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     optimizer, \u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m results \u001b[39m=\u001b[39m train(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     valid_dataloader\u001b[39m=\u001b[39;49mvalid_dataloader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mcriterion,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     network\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrule\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBayesian_dummy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# Return the loss as the objective to minimize\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aritr/OneDrive/Documents/Coding/Python/NLP/dep_project_RB/dummy.ipynb#X40sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mmin\u001b[39m(results[\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m]), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[1;32mc:\\Users\\aritr\\OneDrive\\Documents\\Coding\\Python\\NLP\\dep_project_RB\\engine.py:177\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, valid_dataloader, optimizer, loss_fn, epochs, device, scheduler, model_name, network)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m--> 177\u001b[0m     train_loss, train_acc, train_precision, train_recall, train_f1 \u001b[39m=\u001b[39m train_step(\n\u001b[0;32m    178\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m    179\u001b[0m         dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m    180\u001b[0m         loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m    181\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m    182\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    183\u001b[0m         acc_fn\u001b[39m=\u001b[39;49macc_fn,\n\u001b[0;32m    184\u001b[0m         prec_fn\u001b[39m=\u001b[39;49mprec_fn,\n\u001b[0;32m    185\u001b[0m         recall_fn\u001b[39m=\u001b[39;49mrecall_fn,\n\u001b[0;32m    186\u001b[0m         f1_fn\u001b[39m=\u001b[39;49mf1_fn,\n\u001b[0;32m    187\u001b[0m         network\u001b[39m=\u001b[39;49mnetwork,\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m     test_loss, test_acc, test_precision, test_recall, test_f1 \u001b[39m=\u001b[39m test_step(\n\u001b[0;32m    191\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m    192\u001b[0m         dataloader\u001b[39m=\u001b[39mvalid_dataloader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m         network\u001b[39m=\u001b[39mnetwork,\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    202\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(test_loss)\n",
      "File \u001b[1;32mc:\\Users\\aritr\\OneDrive\\Documents\\Coding\\Python\\NLP\\dep_project_RB\\engine.py:36\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, loss_fn, optimizer, device, acc_fn, prec_fn, recall_fn, f1_fn, network)\u001b[0m\n\u001b[0;32m     33\u001b[0m train_loss, train_acc, train_precision, train_recall, train_f1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[39m# Loop through data loader data batches\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfor\u001b[39;00m batch, (y, X, offsets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     37\u001b[0m     \u001b[39m# Send data to target device\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     X, offsets \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), offsets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device) \u001b[39mif\u001b[39;00m network \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m y\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\aritr\\OneDrive\\Documents\\Coding\\Python\\NLP\\dep_project_RB\\data_setup.py:67\u001b[0m, in \u001b[0;36mcollate_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     64\u001b[0m     offsets\u001b[39m.\u001b[39mappend(processed_text\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[0;32m     66\u001b[0m label_list \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(label_list, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\n\u001b[1;32m---> 67\u001b[0m offsets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(offsets[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mcumsum(dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     68\u001b[0m text_list \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(text_list)\n\u001b[0;32m     70\u001b[0m \u001b[39m# return label_list.to(device), text_list.to(device), offsets.to(device)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK\n",
    "\n",
    "from data_setup import get_dataloaders\n",
    "from model import ClassificationNetwork, RuleNetwork\n",
    "from engine import *\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    train_dataloader, valid_dataloader, vocab_len = get_dataloaders(\n",
    "        \"rule_data.pkl\", params[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # Initialize your model with the given parameters\n",
    "    model = RuleNetwork(\n",
    "        vocab_len,\n",
    "        params[\"embedding_dim\"],\n",
    "        [params[\"num_nodes\"], params[\"num_nodes\"]],\n",
    "        43,\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"min\", patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    results = train(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        valid_dataloader=valid_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=criterion,\n",
    "        epochs=1000,\n",
    "        device=\"cuda:0\",  # type: ignore\n",
    "        scheduler=scheduler,\n",
    "        network=\"rule\",\n",
    "        model_name=\"Bayesian_dummy\",\n",
    "    )\n",
    "\n",
    "    # Return the loss as the objective to minimize\n",
    "    return {\"loss\": min(results[\"test_loss\"]), \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.0001, 0.01),  # Learning rate\n",
    "    \"batch_size\": hp.choice(\"batch_size\", [4, 8, 16, 32, 64, 128, 256]),  # Batch size\n",
    "    \"embedding_dim\": hp.choice(\n",
    "        \"embedding_dim\", [64, 128, 256, 512, 1024, 2048]\n",
    "    ),  # Embedding dimension\n",
    "    \"num_nodes\": hp.choice(\n",
    "        \"num_nodes\", [64, 128, 256, 512, 1025, 2048, 4096]\n",
    "    ),  # Number of nodes\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 5,\n",
       " 'embedding_dim': 2,\n",
       " 'lr': 0.00934641954015768,\n",
       " 'num_nodes': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from data_setup import preprocess\n",
    "\n",
    "exemplar_indices=[]\n",
    "exemplars = pickle.load(open(\"data/samples.pkl\", \"rb\"))\n",
    "# exemplars = [[preprocess(i), j] for i, j in exemplars]\n",
    "data = pickle.load(open(\"data/data.pkl\", \"rb\"))\n",
    "c = 0\n",
    "for i, _ in data:\n",
    "    for j, _ in exemplars:\n",
    "        if j == preprocess(i):\n",
    "            exemplar_indices.append(c)\n",
    "    c += 1\n",
    "# exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exemplar_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplars=[data[i][0] for i in exemplar_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi, I\\'m pretty sure I\\'m suffering from depression, and probably have been for the last 18 months or so. It\\'s only been lately that I feel like the rug has really been jerked out from under me, though. I think it started in mid-2004 when I became seriously ill. Between June and September 2004, I spent three weeks in the hospital, umpteen visits to the E/R, and finally, major surgery to correct the problem. Physically, I\\'m fine now. But we had no health insurance (long story), and when the hospital bills started rolling in, we tried our best to pay the bills, but there was just no way. We ended up filing bankruptcy, I lost my home business, and we very nearly lost our home. Ever since then, I\\'ve felt like I\\'m falling down a dark tunnel, even though the bankruptcy is over with. I was drinking part of this time, but have stopped, thinking that may have been causing the problem. But it\\'s actually become worse since I\\'ve stopped.  Here\\'s my \"symptoms\": I don\\'t usually have trouble falling asleep, but I can\\'t seem to stay asleep for more than a couple of hours. It\\'s hard to go back to sleep, so I usually just get up. Weight gain because I find myself eating even when I\\'m not hungry. The \"comfort food\" thing, I guess. I tend to be a \"stress\" eater, and goodness knows I\\'ve had a lot of that lately! I used to enjoy camping and reading books (etc.), but I don\\'t care about any of that anymore. I couldn\\'t sit down now and read a book no matter how much I wanted to. I get through 5 or 6 pages, and totally lose interest. That\\'s depressing in itself, because I used to love reading so much. I have no desire to get out and do anything. I literally close myself up in my spare bedroom every waking hour, only coming out to use the bathroom and to fix supper for my husband. I feel like I have nothing to look forward to. I\\'m not happy, but I can\\'t pinpoint exactly what would make me happy. I honestly can\\'t think of anything. I have no energy whatsoever.  I\\'ve been dealing with a TON of guilt over the bankruptcy. After all, the bankruptcy wouldn\\'t have happened if I hadn\\'t gotten sick, you know? I feel horrible for dragging my husband\\'s credit down along with mine, but the way things were, I couldn\\'t file bankruptcy by myself. Our credit was PERFECT until I got sick. My husband sold a bunch of his personal belongings to help pay bills, as did I, but it was so unfair for him to have lost his stuff on account of me. He\\'s never once complained, but I still feel terrible. I\\'ve also been kicking myself for the loss of my home business. I can\\'t help but feel like if it hadn\\'t failed (I couldn\\'t keep up with it during my illness), that we wouldn\\'t have had to file bankruptcy. I\\'ve never been a crier, but I sure feel like it sometimes. I can\\'t seem to do anything \"right\" these days. Irritable!!! Throwing stuff, being unreasonable, getting mad about petty things. The one thing I haven\\'t experienced (thankfully) is suicidal thoughts. A few random \"morbid\" thoughts here and there, but nothing I\\'d ever act on. Last night I took several online self-tests, and all of them said I had mild to moderate depression. One even said severe. I guess the next step would be to go to the doctor, but I don\\'t have a doctor. My sister is taking Zoloft for panic attacks and a PMS disorder, so I guess I\\'ll ask her who she\\'s seeing. What do they do ... just ask questions similar to the online questionnaires? Or? I\\'ve been reading about all the different kids of medicines, and found the information about Wellbutrin to be the most interesting. Obviously the doctor would know what I need better than I would, but I sure don\\'t want to take something that\\'s going to make me gain even more weight or feel like a zombie. At least one good thing, though. We do, thankfully, have insurance now. Another issue is my husband and family. To them, depression is something that people should be able to \"just shake off\". They don\\'t understand that I CAN\\'T seem to \"shake it off\", otherwise, I gladly would. My husband is supportive in other ways, but he scoffed when my sister started taking Zoloft. To them, if they can\\'t \"see\" the problem, it doesn\\'t exist. I\\'m sure they don\\'t mean to be \"mean\", but they just don\\'t understand. Sorry for rambling. Any suggestions or thoughts?',\n",
       " \"This world just isn't for me\\nI'm scum. Bottom of the barrel. I'm no role model and I don't try to become one. Death is scary but it's the only relief from the burden of having to change. \",\n",
       " 'I wish you knew that...\\nI feel alone. I feel like I have no purpose. Nobody understands me. Nobody understands how much hurt I go through. How it feels to feel like this. To have two faces. To make is seem like I am happy when I feel like ending it all. But Iâ€™m not that good at hiding it anymore. \\n   If only you knew. If only you knew how I feel. If you knew how I cry every time I am alone. How I cry myself to sleep some nights. How it feels to feel like you have no one. Like no one cares about you. Like no one is there for you. \\n   Do you know now why I donâ€™t smile anymore? Why I have a bad â€œtoneâ€\\x9d? Why I always seem tired? Upset? Why I always want to be by myself? Why I donâ€™t want to go out? \\n   I wish you knew. I wish I could talk to you. I wish I wasnâ€™t so scared to talk to you. I wish I could tell anyone about my sadness. About the thoughts I have. About how many times I have thought about killing myself. About why I feel this way. I wish you would be there for me. I wish you would let me know that there is hope. That there are people who love me. That I can live. That I can be happy too. ',\n",
       " 'I don\\'t know what to do anymore.\\nI\\'ve been with my boyfriend for almost 10 years, since we were 16. Things have generally been awesome, but the past couple of years has just consisted of a giant blow-out argument once every few weeks, generally stupid shit (me complaining of not spending enough time together, him not contributing to chores, him complaining of not enough sex) I don\\'t know how to talk to him about our problems without him getting angry and breaking objects and me getting insanely sad and just not being able to communicate as my brain just goes \"YOU\\'RE USELESS! WHY HASNT HE LEFT YOU YET, USELESS FUCKING CUNT\". I just need someone to talk to or something. i dunno',\n",
       " 'Does anyone else have this constant dark feeling lingering over them?\\nRecently my mom died and this dark feeling of hopelessness has been worse than ever before. I don\\'t know what to do anymore. Nothing makes me happy, and I don\\'t want to die but I don\\'t want to exist either. I can occasionally enjoy things, but at the end of the day I just think to myself, \"why am I still doing this?\" Everyone keeps telling me that I have to live for myself and not search for answers through others but I feel like I can\\'t help myself. I feel like no one around me understands what I\\'m going through. I feel like a robot at work, forcing myself to seem okay. I just feel like living with this feeling inside of me is so hard, and maybe genuinely not even worth it. I don\\'t even feel like taking my medicine or eating heathy or anything. I just wake up, go to work, come home and sleep as much as possible. Sorry to rant on here, I\\'m not sure what else to do. ',\n",
       " \"No end in sight\\nI have been depressed for so long.  I just feel a void,dark and empty.  I can  feel the pain of no one loving me. I have zero friends.  I live with toxic parents .I feel like i have been in an endless loop with   no end in sight to the pain I'm in.  I hate my existence. I have done therapy,yoga blah blah. I still feel miserable. I am so unlucky. Can't do anything right.  What's the point if you're not good at anything. Im  below average in life,no money,No job,No talent and no interest. I don't get it.  Why am I even here?\",\n",
       " \"I noticed people who talk to me, sometimes just walk away mid conversation\\nI'm socially awkward and I have a hard time reading cues about where the person wants to take the conversation. Is he genuine with me right now, am I interesting enough? I end up expressing myself in bizarre ways and people just up and leave. I find it incredibly rude almost like I'm a crazy person and they think it's okay to disrespect me. It hurts a lot\",\n",
       " 'I wrote a poem about depression\\nLittle Johny\\'s gone today,\\nLittle Johny ran away,\\nLittle Johny\\'s at the bay,\\nLittle Johny fades away,\\nLittle Johny what\\'d you say?\\n\\n\"I don\\'t want to live another day\"\\n\\nOh tissue tissue how many tears can you soak?\\nDepression is no joke,\\nnot cured by drink or smoke,\\nmore scarier than a stroke,\\nleaves a scar on most folk.\\nThat\\'s the power of one dead bloke.\\n\\nEvery second a man falls down.\\nEvery second someone drowns.\\nEvery second a bullet to the head,\\nleaves someone lying dead.\\nAll you see is one small frown.\\nIn this little shitty ghost town.\\n\\nIt\\'s your fault, where were you?\\nDon\\'t you know that dead hearts bleed too?\\nNow it\\'s our time to go.\\nbecause depression is not for show.',\n",
       " \"I take effexor and I have gained lots of weight. I have become so self conscious of my self. After buying all new clothes and trying to stay in staying I can't feel sexy anymore.  I love my boyfriend and he is great but I don't even feel 100% sexual with him just because I am not as fun as I use to be. \\n\\nI also have dreams that I can't tell if they are real or not. I will tell someone something thinking we had had a conversation I guess it was in my sleep. I sweat like crazy yes, it is gross but what to do?\\n\\nAlso I really need some advice on my boyfriend. He is great supportive but he says it's hard for him to and it takes a toll on him. How can he better handle this, not that he does not do a wonderful job now, but just new advice for his sake. He is not sick with anything and I tell you I get mood swings like crazy. I am not the same girl he met and I so want to be but I can't shake this and I am afraid that it will be here forever. I can't deal with it forever it burns me out.\\n\\nPlease Help\\n\\noh yes I also find it hard to have friends because they do not understand I want to go out but I don't and plus most of the time I don't have the energy.\\n\",\n",
       " \"The only time I can enjoy my life is when I'm watching anime/listening to music\\nEvery time I put on those headphones that allow me to leave this miserable reality, I feel alive. Any other time I feel like I'm just lifelessly struggling to deal with my responsibilities. That's why I listen to music all the time. That's why I only get excited when watching anime. I need fiction to get me through a day or else I go completely numb.\\n\\nSo uhh is this a problem? Or is it normal?\",\n",
       " \"My grandma is dying and life doesn't stop\\nSo I work full-time and go to grad school part-time. I'm busy 7 days a week between work, class, and homework/housework. Normally this is a very good thing for me as it keeps the depression and other mental illness at bay.\\n\\nBut my grandma's dying. She was put into hospice over the weekend and can't do much of anything anymore. I love her so much as my grandparents/mom's side of the family have been the only stable family I've ever had. My grandmother is an amazing woman. She graduated as valedictorian of her college class. She raised 7 amazing children (all college grads and successful in their fields) and has 14 grandchildren and 5 great-grandchildren. She's 90 and has lived an amazing life.\\n\\nIt's still so hard to say goodbye. I've been doing my best to spend time with her and my visiting family but the grief and depression are kicking my ass. I can't do anything. I have a huge paper due this week and work has been insanely stressful. I can't even manage to get my laundry done and I'm buying and eating shit food that is bankrupting me.\\n\\nI don't know what to do. Life won't stop, and I'm so stressed that my whole body is breaking out in rashes and I'm sore all the time. I don't have any time off work since I haven't been there long and we're already short staffed. The end of the semester is coming up and I can't even think much less write a policy analysis.\\n\\nAny advice is welcomed.\",\n",
       " \"I'm quitting my job.\\nI've been working at a department store for about 5 months yet, and I just can't do it anymore.  There's too much to know and I just can't keep up.  People who have been there 3 weeks are telling me how to do things.  \\n\\nI sat down with my manager this morning and explained it all.  The crying in the bathroom, the calling in sick because I'm too scared to go in, everything. She was really nice about it, and I decided to put in my notice.  \\n\\nIt feels like a failure, but I gave it my best try, and it isn't working out.  \\n\\nI feel relieved that I only have 2 more weeks.  I don't have another job lined up yet but I had to get out.  I know that's not a good choice, but the anxiety is so intense there.  \\n\\nI feel like a bit of a failure.  A blind labradoodle could do this job haha.  \\n\\nAh well, can't win em all, right?  \",\n",
       " 'Is it stress, depression or burn out?\\nHey,\\n\\nIâ€™m a diabetic 1 with Autism. For the past 3 years i have felt constant fatigue, no motivation, sleep issues, stomach problems, bad memory and concentration issues.\\n\\nI took antidepressants Cymbalta 60 mg before this during 8 months for major stress and it helped me with almost no side effects.\\n\\nBut after i quit antidepressants Cymbalta 60 mg ,the fatigue, no motivation, sleep issues, stomach problems and concentration issues has been persistent in my life. Now for 3 years.\\n\\nNo matter how much rest i get i feel tired. Even if excercise, eat healthy, do yoga and meditation.\\n\\nMy blood sugar levels has been up and down lately. But even if they are controlled i feel the same. \\n\\nA cause could be that i have worked on a lot of projects during the past 3 years. \\n\\nIt went as far as that i started to get suicide thoughts. I do not feel any ups or downs in my life. Things i used to like to do isles enjoyable. I rather stay at home than being social.\\n\\nI believe i have major depression and i have visited psychologist and psychiatrist who have advised me to take antidepressants.\\n\\nBut the question is if Iâ€™m stressed, have major depression or simply burnt out.\\n\\nI certainly feel that i have a huge major depression. My OCD thought are back and i have been stuck on testing my blood sugar 10-15 times per day during the past year. Which also could bring my stress levels up. The past year has been very stressful for me work wise. But even if i get a weekend of rest i donâ€™t feel rested. I have been to different psychiatrists and psychologist and the conclusion is either major depression or burn out.\\n\\nI barely have the energy to clean my apartment anymore, wash my clothes or think about my hygiene. I feel tearful occasionally on some days. \\n\\nI have thought about taking antidepressants again, particularly Cymbalta since it worked for me the first time. But i have been afraid of taking them. \\n\\nInstead my psychiatrist described me brintellix (Vortioxetine), 10 mg per day for my depression.\\n\\nBut the question is, iâ€™m i stressed, depressed or burnt out? Anyone have been in a similar case? \\n\\nAny experiences of the antidepressants brintellix (Vortioxetine) for major depression? Iâ€™m so afraid of starting to take them, but i donâ€™t want to feel this way of every morning waking up with no motivation for anything that the day brings me. ',\n",
       " 'Hello again everyone, my real name is Mike. I found this site a couple weeks ago but have been pretty busy with the holidays and a new job. So I thought I would post a bit more about myself. This isn\\'t easy since I am not that good at talking about myself but here goes. I am 35 years old and have been depressed as long as I can remember. When I was 12 or 13 years old I remember sitting in my closet in my room with the doors shut writing a suicide note. After talking to a friend of mine about 7 years ago who started going to a therapist and taking Prozac, I decided to go get help. I didn\\'t even know to call it depression until I started talking to him. I was diagnosed with Dysthymic Disorder and have been on and off a couple different kinds of medicines, one of which was Effexor and the other one I can\\'t remember the name but it started with and S. I have a bad habit of remembering every bad thing that has ever happened to me and continually go over and over it in my head. I will bring up something with my brother or sister and they say \"I can\\'t believe you remember that\" and yet sometimes I am lucky I remember my own name. I just started a new job recently and will not have insurance until the end of January. I am seriously considering going back to a therapist and possibly getting on medicines again. I am tired of carrying around this baggage and want to do something about it. There are so many things I want to do with my life and seem held back by my depression. I found this site one night after feeling really depressed over something that I can\\'t even remember now in the hope talking to people who knew how I felt and what I was going through.',\n",
       " 'Do you ever?\\nDo you ever sit there and go \"what the fuck is wrong with me\" because obviously something must be wrong with me for people to not want to talk to me. I know I\\'m not the most emotionally upbeat person but it\\'s hard to hide how you feel all the time and sometimes I just want to be able to tell someone. ',\n",
       " \"Help!\\nI just want to cut, and cut, and cut.  It feels so good.  So far I have managed to maintain some sort of self control and only cut my wrist and genitalia.  I'm afraid because I have the whole weekend to myself except for a few things I'm doing with friends.  My mind is full of strange thoughts.  I'm sorry I was so descriptive.  I really would like it if someone would respond.  I'm not seeing my therapist until Wednesday and I got into a big fight with my husband on Friday about my depression.  He said I make him depressed.  He doesn't know how to handle me.  He thinks that depression is a choice not a disease.  He wants me to talk to him about my problems like I talk to my therapist. I can't.  He says things like I throw him a guilt trip.  He is one of my problems.  He damages my self -esteem.\\n\",\n",
       " \"A little less sad tonight\\nI really don't know what to say anymore. I can't say anything about stuff I love to people without feeling like I am a burden to them.\\n\\n\\n\\nTonight I landed my first kickflip from the street on to the curb. I've been skating for about 10 years now and never been able to do that until now. It may not seem like a lot but it's a challenge that I've faced with fear most of my life. I'm so happy right now I have no one to share it with so I thought this would be the best place to share it with. I normally don't feel happy anymore or generally feel content but right now I do and I want everyone to share something that they're proud of or something that made them hopeful or brought them comfort in a time of misery.\\n\",\n",
       " 'Hi, sorry if this turns into a lengthy post, but I could do with some advice, and I think this is probably the best place to ask. Firstly, I should point out, I am a 19 year male. My problem is that I am concerned by the physical effects the depression is having on me. I have probably been suffering from depression for several years, but I only sought medical help last November after I had a breakdown. The doctor has prescribed me with Fluoxetine, and I am taking 40mg per day. I originally started off on 20mg per day, but after I told the doctor what I will tell you now, about 2 months ago, he \"upgraded\" my status to severe depression, hence the increase in dosage. I can appreciate that my mindset is slightly biased towards feeling negative, and expecting the worst, but nevertheless I can\\'t help but feel my symptoms might be the result of an even more serious problem. The medication has helped my mind (even though I still have plenty of lows, they aren\\'t always as severe as they were 6 months ago), yet my physical health is deteriorating to the point where that\\'s the main factor preventing me from accomplishing day to day tasks. My symptoms are as follows (and they may be generalized, I\\'ll admit): Constant fatigue, even after plenty of rest; constant aching throughout the body; generally feeling very weak, lacking energy and strength, e.g. carrying shopping home can leave my arms in agony; loss of coordination, I have become very clumsy, always knocking glasses over, or dropping things for no reason; occasional loss of balance, usually when getting up, or starting to walk from a stand still; joints are feeling slightly stiff, they still move, but I can feel more resistance than normal and they get very tired very quick if was to use them, or even just hold my arms straight out in front of me; and for me, the worst symptom is an almost constant tremor in both hands and arms, which sometimes spreads throughout the body. If I attempt slow, fine movements with my hands, the tremor becomes very clear and results in jerky movements. Also, I like to exercise, yet after all exercises, even low intensity ones, the trembling seems to intensify. I doubt it is anxiety, as I still shake when I am calm, relaxed and immobile. I know I can\\'t expect I diagnosis, but I would appreciate it if anyone can tell me if the severity of the physical effects I am experiencing, is quite common, even when the mental state is much more relaxed and beginning to recover? Thanks for any help',\n",
       " \"I just recently got a test back in one of my classes and saw that I totally failed it. Now I'm worried about this class and I can't afford to repeat anything, literally and figuratively. I'm older so I don't have the time to go screw around and repeat classes and I'm sick of school as it is. I hate the uphill battle, just barley hanging on. Why? I did great in high school with a 3.5 GPA. and I made straight A's a couple times in middle school. Other people breeze through school and it makes me angry that it's such a struggle for me just to get C's in college. I'm sick of this I want it all to end, and I still have a whole year after this semester. And even that's not a guarantee the way things are going. Just needed to rant.\",\n",
       " 'Sometimes I wonder if Iâ€™m just a lazy fuck and not depressed at all\\nMy dad just scolded me over the phone because I was still in bed and its 2PM. My parents has been very nice and understanding since I told them about my depression. But when he said something along the lines ofâ€\\x9dno one stays in bed all day, everybody has to get up, you have friends who take the bus to work and work all night long. Why should you get special treatment? The only one who can change how you feel is you. And if you want to feel better you canâ€™t play games all night and sleep all dayâ€\\x9d\\n\\nIâ€™m still in bed, havenâ€™t eaten anything, havenâ€™t done the dishes like I promised i would, havenâ€™t cleaned my room. Iâ€™m worthless.',\n",
       " 'Does therapy help???\\nFor 5 years I have had the same problems, I am socially withdrawn and really not happy. I really want to change, but every time I think about the life I want for myself it seems absolutely impossible, and I continue the cycle. Does therapy really help?',\n",
       " \"i know you wouldn't have guessed it by my name i am 18 i have recently become a paraplegic (paralyzed from the chest down) this is down to unbelievable circumstances misdiagnosed several times  misreading of  results and MRI scans i was in a relationship before the injury for 3 years with a lad similar age. i did sport and exercise science at college and was a well known sports person for several different sports. As well as losing the use of my legs and other bodily functions, my boyfriend cheated on me whilst seriously ill i now feel highly vulnerable and lost as my whole life as i once new it has been taken away not only this but my mom within in herself has so much to cope with as well as watching the pain i go through i nearly died 4 times including boxing day and christmas day. i am lucky in the fact i have plenty of friends and family which support me the only problem is no one understands i have always been very strong minded and kept my chin up and  a smile recently as my 18th birthday has just passed i have realized what i have lost, and how i have 2 sit and watch all my friends getting on with life the doctor is always advising me to take my tablets and antidepressants but i choose not to. Although by reading this you will not be able to see how much i have to deal with. i feel as though i am becoming a burden to my family and i really feel i need someone unrelated to my social surroundings to give me a small nudge back to feeling more like myself. i have always had boyfriends before my injury but now it is impossible for me to see myself with anybody again. Due to the fact using the unfortunate words i am now a wheelchair user. can someone please help me.\",\n",
       " 'I am in a state of emptiness. Recently, I was in a relationship with someone who didn\\'t love me, but I loved them intensely. I had hoped eventually she would turn around, but for that time, I found deep meaning in her. Now it\\'s over, and she doesn\\'t seem to care. Meanwhile, I am in hell. It\\'s like the past four years of my life were just a dream. I feel humiliated, used and defeated. I feel like I have nothing. I lost my job, stopped going to school, and was forced to move back in with my parents. I feel like a total failure and completely worthless. I feel like I am being punished by God, and there is no way out. Even in my dreams I can\\'t escape the pain. I can\\'t see the good in anything anymore. Everywhere, I see pain and suffering, with no hope. Even when I meet incredible people who am in awe of, I feel almost envious, knowing that I am a worthless, useless nothing in the presence of angels. I still have some friends, but have grown distant from them. Seeing their success, no matter how minute, just reminds me of my worthlessness. I have nothing to offer the world. I am without purpose.\\n\\nEven when I have enough things to distract me, my ex is still in my dreams or nightmares every night. Before I met her, I got my life on track, was kicking ass in school, and headed for something. I know I can\\'t blame her, but I just feel so stupid for getting caught up in her drama for no reason. What a waste.\\n\\nI don\\'t know. I\\'ve been reflecting on my life a lot. I\\'m 27 and have nothing to show for it. I feel like I have to make drastic changes. Liquor and drugs are not doing me any good. I don\\'t know what the hell I want anymore. I\\'m so lost, sometimes I don\\'t even know if I\\'m really alive.\\n\\nI feel like right now, I have to settle for mediocrity and hope for simple low - stress job that pays the bills, but I don\\'t have any source of joy or anything really to look forward too.\\n\\nI\\'ve pinpointed that the major source for depression and life problems as been my self-hatred. It manifests itself in many forms, including obsessive feelings of inadequacy and incompetence, paranoia, being overly - sensitive to criticism, feeling worthless, hopeless, and giving up too easy because of those feelings.\\n\\nWhere did all this self - hatred come from? I don\\'t want to blame my parents at my age, but I know that how they treated me when I was younger affected me and it still lingers in my psyche. They always employed the shame approach to parenting. Anytime something happened, my father would curse at me, and lecture me on how worthless I was. He would glare at me, and I just had to sit there and listen. Sometimes I would start crying, and then he would yell at me for crying, and say shit like \"look at you, you\\'re not even ashamed.\" Shame was apparently a virtue. My mother once told me when I was 8 that she wanted to have a third child, but because I was such a disappointment, she decided against it. They hit me frequently, with a bamboo stick. I remember they would constantly compare me to their friend\\'s children, praising them and ask me why I couldn\\'t be more like them.\\n\\nSo all this happened, but I can\\'t change the past. I don\\'t want to dwell and blame my parents for their mistakes, no one is a perfect parent. I just want to get over these feelings of self - hatred and get my life in order. I\\'ve identified the source of my depression, but haven\\'t been able to change all those feelings and thought-processes that have been ingrained in me since early childhood. Does anyone have any advice on how to overcome these feelings, and gain a sense of confidence and direction back?\\n',\n",
       " \"I hope this posts ... I am not feeling good at all. I feel as if I am an outer shell filled with fire that I can not let out. It hurts so much and I don't know what to do. I keep praying but I can't seem to be able and control it. I don't feel worthy. Even this post will probably be deleted. It probably will be because all I ever do is never good enough. I try to try but nothing ever happens. I go to therapy, she said I have 14 personalities ... I hate myself so much right now. I agree not to get on one forum then I go to another ... am I addicted to sadness? Grief? I hate myself so much. I am sick of myself and I just want to scream. Why do I think this way ...\\n\\n\",\n",
       " \"It's been a while since I've written and I decided to start off again with a bit of a bang. I'm going to post my story of beginning recovery, this is to try and cheer some people up and to let them know that they could do it too. Well now, where to start.... About a year and a half ago (the beginning of Grade 8) is when I started to notice the changes. Not physical but emotional. I had been told many times that this was very normal for teenagers so I thought nothing of it. There was an incident with a rather immature and nasty young man, who I just happened to have an ENORMOUS crush on. Anyway we both kind of lead each other on for a while but nothing happened, than he meet this girl at a hockey game. He immediately started talking about her all the time and how he really loved her so much.  I began to feel more and more useless. After that I started rolling down hill like a circle of cheese, and by March of that year I was quite close to suicide at several occasions. This is when I started noticing that this might not be normal and tried many self-help methods, all of which failed. I thought Grade 9 could be a new beginning, it wasn't. My grades were starting to slip and I just couldn't be happy anymore. Finally after a close call in December I told my mum all about my failed attempts and how I was feeling. She agreed to get me a new doctor. I visited her and I got diagnosed with depression and was prescribed Cipralex. Of course that took a while to work too which involved me missing a lot of school, falling really far behind, and having several more close calls. Finally I started feeling a little better and was even stable enough to start going back to school and going to visit my family in the East. Well I just got back from that trip and I have never felt better. I am ready to face the world. That trip was just what I needed. When I get through these next weeks until my birthday all I can see is a varely clear path ahead! For the first time in a year and a half I can concentrate and tolerate less than perfect circumstances. I hope everyone here will feel this way, you can if you just push through! Power to us! Rock on!\",\n",
       " \"Hi, i joined earlier last year in quite a bad state. doubt if anyone will remember me!\\n\\nI kind of  sort of 'snapped out' of it if that's at all possible. I realized i wasn't just upset, but i was never diagnosed as depressed, so i'm so sorry if I'm offending anyone by posting here. \\n\\nBut you guys really helped me when i was last here.\\n\\nFrom the start of this year i've been so 'down' it's unreal. At first i put it down to stress of january exams but even though the exams are over i am still so miserable (for want of a better word). I have a great boyfriend who i love very much and vice versa. we have been going out for about 6 months and everything is going great, but my moods are starting to put a strain on him (tom).\\n\\ni have put it down to the fact that I am really low on self esteem at the moment for some reason, but i don't want get up in the morning because i just don't feel i can face anyone. i LOVE shopping, it makes me feel so good, but it doesn't do anything for me now. I get a bit of Seasonal Affective Disorder, and even sun showers haven't cheered me up.\\n\\nBecause i'm so down i've become really irritable lately, like permanent PMT and like i said, i think its really starting to put pressure on tom, because he's always on the receiving end of my moods.\\n\\nI don't want to mess everything up or slip too far down the depression road again.\\n\\nI just don't know what to do or how to explain things to tom (my boyfriend).\\n\\nAny words of wisdom you could offer me? i think things really might be in trouble.\\n\",\n",
       " 'When I was 16 I was raped by my boyfriend. I was a virgin at the time and it was really hard to accept that this had happened to me. I thought that somehow if I didn\\'t say anything to anyone or raise a fuss about it it wasn\\'t true. But it only made it worse. When I tried to break up with him he tried to kill me. We worked together and that made it really difficult to avoid him. He told all his friends at work that I had \"given it up to him\" which wasn\\'t true. I was mocked and teased by the people that i felt had grown to be my family. \\n\\nNow, five years later it is still something that hangs tightly in my mind and heart. It is very hard for me to trust anyone and good relationships have been few. I still look over my shoulder thinking he is going to be there again, following me, stalking me, preying on me.\\n\\n',\n",
       " 'sometimes i hate myself and its really hard to fight\\nsome times i can ignore it but other days i really am struggling ',\n",
       " \"Anyone else get more depressed than usual on your birthday?\\nToday is my birthday and I hate it. For the last 5 years or so I've been especially depressed on my birthday. Guess it just reminds me that I was born.\\n\\nEDIT: Please don't say happy birthday, it makes it worse. Thanks.\\n\",\n",
       " \"Hi. I'm new, and I'm not really sure where to start in the forums! I've been treated for depression since I was 17 years old, and then misdiagnosed with ADD when I was 17 and that started a downward spiral with abusing Ritalin (snorting about 5-10 times my dosage) for about 5 years. That's over with. I battled with anorexia as a direct result of the Ritalin, and got down to 95 lbs. Then I ballooned up to 185 lbs. I lost 45 lbs. a few years ago and I've kept it off ever since (I've never gone above 144). I've been on Prozac since I was 17, and just recently switched to Cymbalta at the suggestion of my psychiatrist. I felt funny on it, and committed the ultimate sin: I stopped taking my medicines a week ago. I know that's bad. The last two days have been a nightmare, and I even contemplated swallowing a bottle of pills at one point. My fiance is the most amazing person in the world, and I'm literally killing him with my sickness. I don't know what to do. I've been hysterically crying and inconsolable for about a week. Yesterday my fiance called the psychiatrist on-call with our medical program, and she said I needed to take the Cymbalta, so I did take one pill yesterday and again this morning. I do feel weird, though. At the center of it all is my weight gain; I gained about 10ish pounds (I weigh 140 at the moment), and NONE of my clothes fit me. I feel like a disgusting fat blob all of the time. Just one look in the mirror sets me off and I'm HYSTERICAL for the rest of the day. All I want to do is sleep. SLEEP and never wake up. I've had an eating disorder in the past, and I know I can't fall back into it (the anorexia was a direct result of the Ritalin), but I am just hysterical, depressed, and I feel disgusting all day long. My clothes don't fit and I feel fat and disgusting and horrible and I want to cry and rip out my hair. I'm thinking this weight has a LOT to do with my recent downward turn in my depression, but I just can't lose the weight. I work out constantly and follow a really healthy/strict eating plan, and nothing is working. I'm just reaching the end of my rope, here. And to top it all off: I'm getting married next month. I need help...I've called my psychiatrist, but I don't know what to do. I don't see a psychologist; I think I need to, but I've had bad experiences in the past. I think I just need help and guidance and a way to lose this weight and feel good about myself. Hoping this forum will help.\",\n",
       " \"I'm not exactly sure what the topic is (kind of vague), but for me: Depression started around the age of 11 (I'm almost 23 now), one summer I was sitting in my room, laying on my bed, listening to music, and gazing out the window, and it kind of hit me like a ton of bricks. I just felt like I didn't even exist.  Like the world outside my window didn't exist. Like the sun would never shine again.  Like life itself was pointless. I tried to talk to my mom about it, but she obviously didn't understand what I was saying to her, she told me I needed to do more work around the house to keep myself busy... look at me now...\",\n",
       " 'I have my ECT consultation today and Iâ€™ve never felt more alone.\\nI have a good support system around me but somehow Iâ€™ve never felt more alone than at this moment. Nobody else that I know has had depression as severe as mine, or has ever been in this position where the choices are: be miserable forever, or zap your brain which might seemingly cause permanent memory damage and may not even work after all that.\\n\\nLast night was the first time Iâ€™ve felt really suicidal in a while (mostly Iâ€™ve just been miserable and havenâ€™t even been motivated to plan my suicide). Why go through all these awful, invasive treatments when I could just end it quickly and quietly and then everyone else could get back to their lives? I feel like an immense burden. \\n\\nIâ€™ve been severely depressed for the better part of 2 years at this point. It has totally disabled me, robbed me of a career I loved, and made most things difficult or unbearable. Iâ€™ve tried 7 different drugs to try and get it and my anxiety under control. No cigar. \\n\\nI donâ€™t necessarily want to die, I just donâ€™t want to live, if that makes sense. I feel as if my husband, friends, and doctor are keeping me artificially alive with all these treatments that donâ€™t work, when in reality, Iâ€™m that painfully sick dog that needs to just be put down already. \\n\\nI never thought the potential for another treatment would actually make me more suicidal, but here we are. Thanks for reading.',\n",
       " 'What Depression is Like For Me.\\nHaving depression is like sailing with a hole inside your boat and only a bucket to keep you afloat. Depression is wanting to have friends but are too anxious to go out and stay a while longer. Depression is forcing yourself out of bed, not for yourself but those who count on you. Depression is having an empty feeling inside your chest while your head is crowded with self loathing. Depression is where people tell you \"it will all get better\" but only hearing the silent whispers of the wind. Depression is where you slowly lose your personality, joy, and interest for those things that you have used to love. Depression is where a stranger tells you \"I\\'ll be here for you\" and you take it as an insult.\\n\\nDepression is where you hurt, crawl on the floor crying, grabbing on to dear life alone in your dark room with your thoughts, too tired to do anything and then go to bed only to rise again to pretend that nothing happened.\\n\\nTo pretend, nothing happened. ',\n",
       " 'Long text, I just want an audience.\\nWhy havenâ€™t I killed myself yet?\\n\\nItâ€™s not the title, but a question that lingers on the back of my head every second. Itâ€™s a common trend that people live for other people but what do you do when you have no one to live for? I never had anyone to live for. Itâ€™s not a huge problem really, its another lingering question or jealousy I have whenever I meet a person. Every single one of them have someone who cares for them, even a stranger would care for them. It makes me feel jealous that no one would care for me or even bother to be nice to me unless I open my mouth and say I have cancer or they knew about any of my situation. People donâ€™t care, they pity me. They realize, then, that Iâ€™m a hopeless situation and they canâ€™t help me at all because they really canâ€™t. No one can.\\nTherapies donâ€™t help. My therapist knows all of my situations are out of my control and he wants me to move forward with my life. Itâ€™s not really a bad idea, itâ€™s the smart thing to do especially being someone in my situation. I just donâ€™t know why I canâ€™t. Every single being on the planet wants me to graduate already but my body doesnâ€™t agree with me. Itâ€™s always either Iâ€™m too sad/ stressed/ depressed/ ashamed/ anxious/ lazy to go to school. Nothing helps. Not even the 30mg of Lexapro helps. My school counsellor is doing the nice â€œI get it, I understandâ€\\x9d approach with me and it makes me feel better but it really does make me feel guilty.\\n\\nI just donâ€™t understand why are people act so shitty with me? Is it my face? Is it because no one cares and all I have is literally myself? Is it because Iâ€™m dying soon? What is it really? I just want to fucking die but I cant just let myself slash my throat or swallow a lot of pills because Iâ€™m a coward. I want the easy way out. Why isnâ€™t cancer an instant death? Why arenâ€™t suicide booths similar to futurama a thing? \\n\\nI give up. I just want to die. \\n\\nP.S. Fuck people who say theyâ€™re afraid of rejection because hunty what world do u fucking live in that not one single person has said no to you? \\n\\n\\nP.P.S. Fuck you E, Iâ€™m sorry I disappointed you that I didnâ€™t die yet even my diagnosisâ€™ average life span is 6 months. Donâ€™t worry, itâ€™s coming. And I surely fucking hope karma is real. Itâ€™s fucking people like you who deserve such things and what do you fucking get? A nice family, a handsome face, people who likes to be around you. Youâ€™re one of the people who deserve my revenge and yet Iâ€™m being like a pussy who is letting a fake thing called â€œkarmaâ€\\x9d take care of everything. In this fucking lifetime, not a single one got any karma. (or maybe my parents did by god giving me cancer but idk they just labelled me as a liability and now has dropped me like a hot potato so thatâ€™s really more on me than on them)\\n\\n\\nPLEAAAAAAAAAAAASE I just want to die. Please just kill me. \\n',\n",
       " 'Ready to just say \"fuck it\"\\nI can\\'t take it anymore. I\\'m an atheist democrat living in a family of hardcore christian conservatives. Ive made it clear that Im a democrat, to which they just call me a snowflake all the time, which just makes me resent them even more. I dont dare tell them Im am atheist, but Im thinking about it. I dont have anything to live for anymore. \\n\\nI dont have any idea what to do with my life. I dont have any special talent or skills. The only hobby I really have is video games, but Im horrible at those. I\\'ve raged so hard at Battlefield that I broke down in tears and drooled everywhere, and anytime I play an RPG I restart so many times that it makes me not want to play anymore. \\n\\nAll my friends have at least an idea of what they want to do, but I dont even have the beginnings of a clue as to what I want to do with the rest of my life. And to be honest, Im scared of becoming an adult. Managing a budget, doing taxes, paying bills, and shit like that. Im not fucking prepared for it. If I cant deal with the stress of a fucking video game, then I sure as hell cant deal with a job. \\n\\nMy parents are also hounding me to get a girlfriend. I dont even have a friend who is a girl. All my close friends are male. Hell, the closest Ive came to having a full conversation with a girl is helping her with spanish homework. I aint gay, I just cant talk to girls. The social anxiety makes me want to vomit if I even think about approaching a girl. \\n\\nSo Im ready to fuck it all and just tell my parents Im an atheist.  Let them send me to whatever fucking counseling they want. Let them disown me and have to go live with a friend. I have no future other than a goddamn walmart greeter at this point',\n",
       " \"ok, im having problems speaking to people about this so i thought I'd try the internet\\n\\nI'm 20 years old and a student, about 8 years ago i had an experience that changed my life, my dad got himself a new girlfriend – that's where the problems started i won't go into the tedious details (unless you want me too) but basically this led to me developing an eating disorder and a serious case of social phobia (i used to lock myself in my room and only go downstairs for tea: literally) this lasted about 6 years - i suppose 6 years being alone would be enough to disrupt anyone's life .. anyway i have got over the eating disorder but the social disorder remains despite all my best efforts\\nLet me give you a few examples of how my life is at the moment. i can't even go to the local supermarket because the staff always want to socialize, i have to force myself to go down the launderette cause i don't want to be around other people, the list is endless. can you imagine what its like being a student and not wanting to be in social situations? my social skills have seriously degraded over the years and i desperately want at least one friend, but i don't know where to start. i moved out because i couldn't cope but now i lock myself away 24 hours a day and read my lecture notes over the internet\\n\\nBasically i have no friends, no money and an addiction to alcohol(i was stupid, thought it would solve my problems) and today I've found out that the bank won't lend me any more money, and i can't get a job - working with others everyday is just inconceivable. i have a counseling appointment on the 3rd of may but i can't wait that long. things couldn't get any worst\\n\\ni don't know why I'm writing this, but i guess i want someone to talk to that doesn't know me, so i suppose I'm looking for a penpal or just someone to talk to. and to make things worst i think i might be depressed - why else would i be crying while I'm writing this\\n\\nany views on this would be appreciated, i can't keep going like this.\\n\",\n",
       " \"I feel like a bit of a jerk posting here when I haven't been around to support any of you, so I hope someone out there is feeling generous tonight.\\n\\nMy background story consists on being depressed on and off for the past 12 years or so, I am now 20. Finally started antidepressants in October. By December the side effects were not stopping and I stopped taking them because I always felt sick to my stomach .... and a person can't work that way. So I stopped them .... I have not been to the doctor since and have been feeling on top of the world.\\n\\nEven my co-workers have noticed a big difference and my manager has been giving me a lot of opportunity at work (training new hires, being on committees etc)\\n\\nBut ....\\n\\nthen today hit,. like a ton of bricks to the head.\\n\\nI'm tired, cranky, I cant stop crying and am trying very hard not to cut myself. I hate everything and am just beside myself.  I don't know what to do. I have the energy from when i felt great but I feel like crud and have no where to constructively place this energy.\\n\\nI don't think I can go back to my doctor ..... she'll just tell me to take the drugs and stop whining.\\n\\nI'm not so sure I'm depressed. Maybe I'm normal and it's the rest of the world messed up? That's a whole other story though.\\n\\nCan anyone here honestly say that medications work and you can be a functional person? I mean ... will this ever end so I can get some peace?\\n\\nAny opinions or insight would be appreciated.\\n\",\n",
       " 'Massively depressed and looking for low-cost help\\nHi there,\\n\\nI am not the type of person to turn to the internet for self-help, but I\\'m at a loss about what to do and I feel like I have nowhere to turn. I\\'m 29 y/o F with a history of depression since age 14. I\\'ve been on and off about 3-4 different type of antidepressants and in and out of therapy since initial diagnosis, so neither concept is really foreign to me.\\n\\nI\\'m looking for a low-cost way to talk about my symptoms, even an alternative to real \"talk therapy\". I was in a MSBP-type situation where as a young adult I was forced into therapy when I wasn\\'t ready, and made to discuss what should have been patient protected information with my family after my sessions, so I\\'m hesitant to \"get into\" that again after the experience.\\n\\nAdvice? Help? Anything? ',\n",
       " 'Being told by my brother making friends isnâ€™t that hard for someone my age bothers me every time I think about it\\nItâ€™s pretty damn hard when I have no hobbies to relate to anyone except stay in my room on my computer because Iâ€™d rather just be away from everyone so I donâ€™t make it hard on them. \\n\\nAnd being told to go to clubs by my parents. I just feel more of a disappointment when I tell them thereâ€™s nothing Iâ€™m interested in and that I also am too afraid to still be alone when I join one. Iâ€™ve done everything I could in my life to try to fit in or make new friends, and thereâ€™s a reason why I have no motivation to try any of these anymore. And then they say it wouldnâ€™t hurt trying. That couldnâ€™t be any further from the truth. If I try to fit in again and still donâ€™t have anyone to talk to, Iâ€™m gonna hate my life even more. Itâ€™s like Iâ€™m trapped. I canâ€™t do anything to please me. One day I want friends, then the next I donâ€™t want anybody at my school to bother me because I know they donâ€™t really want me to be there friend. \\n\\nWhy does depression always put you in a trap that you canâ€™t get out of? Thereâ€™s no solution except accept your problem and just be an introvert and ignore everyone.',\n",
       " 'They say forcing yourself to be social is a good thing, but does it pain anyone else to see how much your friends are progressing in life?\\nMy friends have all become successful and wealthy. They go on expensive holidays and date duper attractive people. I try to force myself to hang out with them but the constant obvious comparison is getting me really down. ',\n",
       " \"Today, I feel so horrible, it makes me want to die\\nI made a fool of myself at work, felt so stupid after the meeting so I left work, told the boss I'm sick. Spent the remaining afternoon in bed. \",\n",
       " 'I started doing my homework\\nnormally I never did my homework (high school) and I started doing it second semester just because. I never had the motivation so i guess iâ€™m getting something done. ',\n",
       " \"Wishing I could cry, but can't\\nIt's really hard, trying to sit, and think about something, and having that feeling where you want to cry, but the tears just won't come out.\\n\\nThe worst thing about this is that my day was going well. I don't have any issues.\\n\\nWhy do I feel like this? Makes the day look useless, makes my life look useless.\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(exemplars,open(\"data/exemplars.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the vocabulary...\n",
      "Done!\n",
      "Creating dataloaders...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from utils import load_model\n",
    "from model import *\n",
    "from data_setup import get_dataloaders\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "rules = pickle.load(open(\"data/rules.pkl\", \"rb\"))\n",
    "data = pickle.load(open(\"data/data.pkl\", \"rb\"))\n",
    "\n",
    "train_dataloader, _, test_dataloader, vocab = get_dataloaders(\"data.pkl\", 16)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "itos = vocab.get_itos()\n",
    "\n",
    "\n",
    "# classification_base_model = ClassificationNetwork(vocab_size, 1024, [128], 2, None)\n",
    "# rule_base_model = RuleNetwork(vocab_size, 512, [256, 256], 43)\n",
    "\n",
    "# classification_saved_model = (\n",
    "#     \"classification_lr0.007_bs128_ed1024_nhn[128]_pat5_best_model.pth\"\n",
    "# )\n",
    "# rule_saved_model = \"rule_lr0.0003_bs128_ed512_nhn[256, 256]_pat5_best_model.pth\"\n",
    "\n",
    "# classification_model = load_model(classification_base_model, classification_saved_model)\n",
    "# rule_model = load_model(rule_base_model, rule_saved_model)\n",
    "\n",
    "# acc = 0\n",
    "# new_data = pickle.load(open(\"data/reddit_7k_data.pkl\", \"rb\"))[1:]\n",
    "\n",
    "# for _, (label, datapoint, offset) in enumerate(test_dataloader):\n",
    "#     pred = make_prediction(\n",
    "#         rules,\n",
    "#         classification_model,\n",
    "#         rule_model,\n",
    "#           ),\n",
    "#         datapoint,\n",
    "#     )\n",
    "#     # pred=torch.argmax(classification_model(datapoint,offset)).item()\n",
    "#     # print(pred,label)\n",
    "#     if pred == int(label):\n",
    "#         acc += 1\n",
    "\n",
    "# acc /= len(test_dataloader)\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.nn.functional.binary_cross_entropy_with_logits(torch.rand((16,2)),torch.ones((16,2)),reduction=\"none\")*torch.rand((16,1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand((16,2))\n",
    "y=torch.rand((43,2))\n",
    "z=torch.rand((16,43))\n",
    "w=torch.rand((16,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((x@y.T).log()*z).sum(-1) * (1-w)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False,  True, False, False, False, False,\n",
      "         False,  True, False,  True, False,  True, False, False, False, False,\n",
      "         False,  True, False, False, False,  True,  True, False, False,  True,\n",
      "          True,  True, False, False, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [False, False, False, False, False,  True, False,  True, False, False,\n",
      "          True, False, False,  True,  True,  True, False, False, False, False,\n",
      "         False, False, False,  True,  True,  True,  True, False,  True,  True,\n",
      "          True,  True, False,  True, False,  True, False, False, False,  True,\n",
      "          True, False,  True],\n",
      "        [ True,  True, False, False, False,  True, False,  True, False,  True,\n",
      "          True,  True, False,  True,  True,  True, False, False, False, False,\n",
      "          True,  True, False,  True,  True,  True,  True, False, False,  True,\n",
      "          True,  True, False, False, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [ True, False, False, False, False,  True,  True,  True, False, False,\n",
      "         False,  True, False,  True, False,  True, False, False, False, False,\n",
      "         False,  True, False, False,  True,  True,  True, False,  True,  True,\n",
      "          True,  True, False,  True, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [False,  True, False, False, False,  True,  True,  True, False, False,\n",
      "          True,  True, False,  True,  True,  True, False, False, False, False,\n",
      "          True, False, False,  True, False,  True,  True, False, False,  True,\n",
      "          True,  True, False, False, False, False, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [False, False, False, False, False,  True, False,  True, False, False,\n",
      "          True, False, False,  True, False, False, False, False, False, False,\n",
      "         False,  True, False,  True, False,  True,  True, False,  True,  True,\n",
      "          True,  True, False, False, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [False,  True, False, False, False,  True,  True,  True, False, False,\n",
      "         False,  True, False,  True,  True,  True, False, False,  True, False,\n",
      "          True,  True, False,  True,  True,  True,  True, False, False,  True,\n",
      "          True,  True, False, False, False, False, False, False, False,  True,\n",
      "          True,  True, False],\n",
      "        [ True,  True, False, False, False,  True,  True,  True, False,  True,\n",
      "         False,  True, False,  True, False,  True, False, False, False, False,\n",
      "          True,  True, False, False,  True,  True,  True, False,  True,  True,\n",
      "          True,  True, False, False, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [False, False, False, False, False,  True, False,  True, False, False,\n",
      "         False, False, False,  True, False, False, False, False, False, False,\n",
      "         False,  True, False, False, False,  True,  True, False,  True,  True,\n",
      "          True,  True, False,  True, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [False, False, False, False, False,  True,  True,  True, False, False,\n",
      "          True, False, False,  True, False,  True, False, False, False, False,\n",
      "         False,  True, False,  True,  True,  True,  True, False,  True,  True,\n",
      "          True,  True, False,  True, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [False, False, False, False, False,  True,  True,  True, False, False,\n",
      "         False,  True, False,  True, False,  True, False, False, False, False,\n",
      "         False,  True, False, False,  True,  True,  True, False,  True,  True,\n",
      "          True,  True, False,  True, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [ True,  True, False, False, False,  True, False,  True, False,  True,\n",
      "         False,  True, False,  True, False,  True, False, False, False, False,\n",
      "          True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "          True,  True, False,  True, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [ True, False, False, False, False,  True, False,  True, False, False,\n",
      "          True, False, False,  True, False,  True, False, False,  True, False,\n",
      "         False,  True, False,  True,  True,  True,  True, False,  True,  True,\n",
      "          True,  True, False, False, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [ True,  True, False, False, False,  True,  True,  True, False, False,\n",
      "          True,  True, False,  True,  True,  True, False, False, False, False,\n",
      "          True, False, False,  True,  True,  True,  True, False,  True,  True,\n",
      "          True,  True, False,  True, False,  True, False, False, False,  True,\n",
      "          True,  True, False],\n",
      "        [False, False, False, False, False,  True, False,  True,  True, False,\n",
      "         False,  True, False,  True, False,  True, False, False, False, False,\n",
      "         False,  True, False, False,  True,  True,  True, False, False,  True,\n",
      "          True,  True, False, False, False,  True, False, False, False,  True,\n",
      "          True, False, False],\n",
      "        [ True,  True,  True, False, False,  True,  True,  True, False, False,\n",
      "         False,  True, False,  True,  True,  True, False, False, False, False,\n",
      "          True,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
      "          True,  True, False, False, False,  True, False, False, False,  True,\n",
      "          True,  True, False]])\n"
     ]
    }
   ],
   "source": [
    "for _, (\n",
    "    y,\n",
    "    X,\n",
    "    offsets,\n",
    "    rule_assigned_instance_labels,\n",
    "    rule_coverage_matrix,\n",
    "    labelled_flag_matrix,\n",
    "    rule_exemplar_matrix,\n",
    ") in enumerate(train_dataloader):\n",
    "    print(rule_assigned_instance_labels==rule_coverage_matrix)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8419811320754716, 0.8419811320754716)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8419811320754716,0.8419811320754716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def joint_scores_from_f_and_w(self, weights, m, f_probs):\n",
    "    num_classes = self.num_classes  # 2\n",
    "    rule_classes = self.rule_classes  # 43\n",
    "\n",
    "    \n",
    "\n",
    "    # Expand dimensions for broadcasting in subsequent operations\n",
    "    weights_expanded = weights.unsqueeze(-1)  # (1,43,1)\n",
    "    m_expanded = m.unsqueeze(-1)  # (1,43,1)\n",
    "\n",
    "    # Create a mask based on weights\n",
    "    weights_mask = (weights_expanded > 0.5).float()\n",
    "    m_masked = m_expanded * weights_mask\n",
    "\n",
    "    # Create one-hot encodings for rule classes\n",
    "    one_hot_rule_classes = torch.nn.functional.one_hot(\n",
    "        rule_classes.to(torch.int64), num_classes\n",
    "    ).float()\n",
    "    one_hot_rule_classes_expanded = one_hot_rule_classes.unsqueeze(0)\n",
    "\n",
    "    # Calculate rule-weight product\n",
    "    rule_weight_product = weights_expanded * one_hot_rule_classes_expanded + (\n",
    "        1 - weights_expanded\n",
    "    ) * (1 - one_hot_rule_classes_expanded)\n",
    "\n",
    "    # Calculate sum of rule firings\n",
    "    sum_rule_firings = m_masked.sum(dim=1)\n",
    "\n",
    "    # Compute the result\n",
    "    result = m_masked * rule_weight_product\n",
    "    result = result.sum(dim=1) / (sum_rule_firings + 1e-20)\n",
    "    result += f_probs\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_rule_classes = torch.nn.functional.one_hot(torch.ones(43).to(torch.int64), 2).float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 43, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_rule_classes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9386792452830188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(test_dataloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I m worthless <unk> I <unk> going to just assume nobody will read this like everything else I post here but here it is I guess . I m a junior in college <unk> I <unk> been single for <unk> years <unk> and I struggle with things everyone seems to get . I <unk> worthless to the world <unk> what do I offer ? I just failed a statistics quiz and went to my car and wept for <unk> minutes . I do <unk> know why . I feel alone all the time <unk> and I feel like I ca <unk> please anyone no matter what I do . I just feel like if I disappeared the world would sigh in relief . It <unk> not like anyone would notice me being gone . I do <unk> have a girlfriend or anything like that <unk> because let <unk> be honest <unk> what girl wants to be with a loser who has to work twice as hard as everyone else . I want to die . It feels like I <unk> on an island by myself and I just want to end it . Just when everything was getting better <unk> here I am <unk> contemplating how alone I am in this fucking world and how I can just get out . '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"\".join([itos[i]+\" \" for i in x])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9475549255846917"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
