{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import pickle\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "RANDOM_STATE = 5411\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "# torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"data/data.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"#[A-Za-z0-9]*\", \"\", text)\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[\\n]\", \" \", text)\n",
    "    # text = re.sub(r\"[.*/]\", \" \", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[preprocess(i[0]), i[1]] for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aritr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(data), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23160"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = lambda x: vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'unk', '>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "glove = pd.read_csv('D:/Downloads/glove.840B.300d/glove.840B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.082752</td>\n",
       "      <td>0.672040</td>\n",
       "      <td>-0.14987</td>\n",
       "      <td>-0.064983</td>\n",
       "      <td>0.056491</td>\n",
       "      <td>0.402280</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>-0.331100</td>\n",
       "      <td>-0.306910</td>\n",
       "      <td>2.0817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14331</td>\n",
       "      <td>0.018267</td>\n",
       "      <td>-0.18643</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>-0.355980</td>\n",
       "      <td>0.053380</td>\n",
       "      <td>-0.050821</td>\n",
       "      <td>-0.191800</td>\n",
       "      <td>-0.378460</td>\n",
       "      <td>-0.06589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.207510</td>\n",
       "      <td>-0.12578</td>\n",
       "      <td>-0.593250</td>\n",
       "      <td>0.125250</td>\n",
       "      <td>0.159750</td>\n",
       "      <td>0.137480</td>\n",
       "      <td>-0.331570</td>\n",
       "      <td>-0.136940</td>\n",
       "      <td>1.7893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16165</td>\n",
       "      <td>-0.066737</td>\n",
       "      <td>-0.29556</td>\n",
       "      <td>0.022612</td>\n",
       "      <td>-0.281350</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.140190</td>\n",
       "      <td>0.138710</td>\n",
       "      <td>-0.360490</td>\n",
       "      <td>-0.03500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.272040</td>\n",
       "      <td>-0.062030</td>\n",
       "      <td>-0.18840</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>-0.018158</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>-0.138770</td>\n",
       "      <td>0.177080</td>\n",
       "      <td>0.177090</td>\n",
       "      <td>2.5882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.42810</td>\n",
       "      <td>0.168990</td>\n",
       "      <td>0.22511</td>\n",
       "      <td>-0.285570</td>\n",
       "      <td>-0.102800</td>\n",
       "      <td>-0.018168</td>\n",
       "      <td>0.114070</td>\n",
       "      <td>0.130150</td>\n",
       "      <td>-0.183170</td>\n",
       "      <td>0.13230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.185670</td>\n",
       "      <td>0.066008</td>\n",
       "      <td>-0.25209</td>\n",
       "      <td>-0.117250</td>\n",
       "      <td>0.265130</td>\n",
       "      <td>0.064908</td>\n",
       "      <td>0.122910</td>\n",
       "      <td>-0.093979</td>\n",
       "      <td>0.024321</td>\n",
       "      <td>2.4926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59396</td>\n",
       "      <td>-0.097729</td>\n",
       "      <td>0.20072</td>\n",
       "      <td>0.170550</td>\n",
       "      <td>-0.004736</td>\n",
       "      <td>-0.039709</td>\n",
       "      <td>0.324980</td>\n",
       "      <td>-0.023452</td>\n",
       "      <td>0.123020</td>\n",
       "      <td>0.33120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.319240</td>\n",
       "      <td>0.063160</td>\n",
       "      <td>-0.27858</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.079248</td>\n",
       "      <td>-0.214620</td>\n",
       "      <td>-0.104950</td>\n",
       "      <td>0.154950</td>\n",
       "      <td>-0.033530</td>\n",
       "      <td>2.4834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.12977</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.18888</td>\n",
       "      <td>-0.004274</td>\n",
       "      <td>-0.106450</td>\n",
       "      <td>-0.258100</td>\n",
       "      <td>-0.044629</td>\n",
       "      <td>0.082745</td>\n",
       "      <td>0.097801</td>\n",
       "      <td>0.25045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2        3         4         5         6         7     \n",
       "0                                                                          \n",
       ",   -0.082752  0.672040 -0.14987 -0.064983  0.056491  0.402280  0.002775  \\\n",
       ".    0.012001  0.207510 -0.12578 -0.593250  0.125250  0.159750  0.137480   \n",
       "the  0.272040 -0.062030 -0.18840  0.023225 -0.018158  0.006719 -0.138770   \n",
       "and -0.185670  0.066008 -0.25209 -0.117250  0.265130  0.064908  0.122910   \n",
       "to   0.319240  0.063160 -0.27858  0.261200  0.079248 -0.214620 -0.104950   \n",
       "\n",
       "          8         9       10   ...      291       292      293       294   \n",
       "0                                ...                                         \n",
       ",   -0.331100 -0.306910  2.0817  ... -0.14331  0.018267 -0.18643  0.207090  \\\n",
       ".   -0.331570 -0.136940  1.7893  ...  0.16165 -0.066737 -0.29556  0.022612   \n",
       "the  0.177080  0.177090  2.5882  ... -0.42810  0.168990  0.22511 -0.285570   \n",
       "and -0.093979  0.024321  2.4926  ... -0.59396 -0.097729  0.20072  0.170550   \n",
       "to   0.154950 -0.033530  2.4834  ... -0.12977  0.371300  0.18888 -0.004274   \n",
       "\n",
       "          295       296       297       298       299      300  \n",
       "0                                                               \n",
       ",   -0.355980  0.053380 -0.050821 -0.191800 -0.378460 -0.06589  \n",
       ".   -0.281350  0.063500  0.140190  0.138710 -0.360490 -0.03500  \n",
       "the -0.102800 -0.018168  0.114070  0.130150 -0.183170  0.13230  \n",
       "and -0.004736 -0.039709  0.324980 -0.023452  0.123020  0.33120  \n",
       "to  -0.106450 -0.258100 -0.044629  0.082745  0.097801  0.25045  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_embedding[','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.zeros((len(vocab), 300))\n",
    "\n",
    "for i in range(1, len(vocab)):\n",
    "    word = vocab.get_itos()[i]\n",
    "    if word in glove_embedding:\n",
    "        embedding_matrix[i]=torch.from_numpy(glove_embedding[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1941,  0.2260, -0.4376,  ...,  0.0920,  0.3863,  0.1174],\n",
       "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
       "        [-0.0828,  0.6720, -0.1499,  ..., -0.1918, -0.3785, -0.0659],\n",
       "        [ 0.3192,  0.0632, -0.2786,  ...,  0.0827,  0.0978,  0.2504]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(embedding_matrix,open(\"embeddings.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pickle.load(open(\"embeddings.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(stoi(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random.sample(data, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.2\n",
    "split_index = int(len(data) * (1 - split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data[:split_index], data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 565)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch,generator=torch.Generator(device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    "    generator=torch.Generator(device=device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_nodes, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.dense_block = nn.Sequential(\n",
    "            nn.Linear(embed_dim, num_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(num_nodes, num_class),\n",
    "        )\n",
    "\n",
    "        self.embedding.apply(self.init_weights)\n",
    "        self.dense_block.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        initrange = 0.5\n",
    "\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            layer.bias.data.zero_()\n",
    "        elif isinstance(layer, nn.EmbeddingBag):\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.dense_block(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 2\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 64\n",
    "model = TextClassificationModel(vocab_size, embed_size, 512, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[ 5.0617e-40,  1.3606e-15, -2.2866e-40,  ..., -4.2641e-17,\n",
       "             3.1035e-39, -6.7272e-22],\n",
       "           [-8.5938e-02,  3.2254e-01,  5.0457e-02,  ..., -3.8322e-02,\n",
       "            -8.9960e-02, -4.4433e-03],\n",
       "           [ 1.2289e-01,  6.0114e-02,  1.8644e-01,  ..., -2.9667e-01,\n",
       "            -2.7566e-01, -1.9465e-01],\n",
       "           ...,\n",
       "           [-9.0253e-06,  6.6264e-05, -3.1442e-04,  ...,  1.6112e-04,\n",
       "             3.4517e-05,  6.5777e-07],\n",
       "           [ 6.4758e-40, -2.0265e-12,  6.6061e-41,  ..., -1.3085e-18,\n",
       "            -3.8977e-39, -4.2688e-40],\n",
       "           [ 4.9056e-05, -3.1769e-04, -2.2225e-04,  ...,  1.2772e-04,\n",
       "             1.0306e-04, -6.7761e-05]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[ 4.9720e-03, -3.8873e-03,  2.8663e-03,  ..., -3.0103e-04,\n",
       "            -5.1750e-04,  5.6413e-03],\n",
       "           [ 1.6180e-03, -5.2169e-02, -1.9099e-02,  ...,  1.8352e-02,\n",
       "            -3.8149e-02, -1.2734e-02],\n",
       "           [ 4.3576e-03, -6.9534e-02, -2.8206e-02,  ...,  2.3572e-02,\n",
       "            -3.8157e-02, -4.8085e-04],\n",
       "           ...,\n",
       "           [ 3.8063e-03,  1.0694e-02,  7.3188e-03,  ..., -8.6752e-03,\n",
       "             9.0609e-03, -6.3648e-03],\n",
       "           [-2.1297e-29,  2.8666e-33,  6.8726e-17,  ..., -3.6290e-19,\n",
       "             9.4901e-35, -2.2122e-16],\n",
       "           [-4.1916e-03,  6.0640e-02,  1.5416e-02,  ..., -2.0623e-02,\n",
       "             4.6257e-02,  1.5298e-02]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-4.8541e-02,  2.5032e-02,  3.0954e-02,  7.3671e-03,  4.6973e-02,\n",
       "            4.6491e-02, -2.4345e-02,  2.4159e-02,  1.1587e-02,  4.0508e-02,\n",
       "           -7.5365e-09,  1.6322e-02,  5.6569e-02, -1.4966e-02, -2.7507e-03,\n",
       "           -1.4014e-02,  4.2039e-02,  2.8920e-02,  1.5983e-02,  4.2507e-03,\n",
       "            1.5304e-02,  6.9926e-02,  4.3077e-02,  3.6175e-03,  2.5455e-02,\n",
       "           -3.6211e-02, -6.3910e-04,  4.2066e-03,  1.1534e-02,  3.2473e-02,\n",
       "            5.7871e-02,  5.2768e-02,  2.0664e-02, -1.9505e-02,  3.6803e-02,\n",
       "            6.0036e-02, -1.9276e-02,  3.8523e-02, -8.9749e-03,  2.9042e-02,\n",
       "            5.4590e-03, -1.5635e-02, -4.5276e-03, -4.4159e-04,  2.7347e-03,\n",
       "            1.1346e-02, -5.3011e-04,  1.8815e-02, -2.0194e-03,  5.5595e-02,\n",
       "           -3.3343e-06,  2.6839e-02,  1.2669e-02, -4.6802e-03,  3.1302e-02,\n",
       "            2.3669e-02, -1.0479e-02,  3.6923e-02, -1.5955e-02, -1.8176e-03,\n",
       "            3.5309e-02,  3.3806e-02, -2.4309e-02,  2.9455e-02, -1.3858e-02,\n",
       "            6.4852e-03,  4.1651e-02,  3.5122e-02,  2.1591e-02,  6.8456e-03,\n",
       "           -7.0643e-03,  2.4218e-02,  3.4863e-02, -1.5273e-02, -8.8954e-03,\n",
       "           -8.7964e-03,  1.0745e-02,  3.9358e-02, -2.1034e-06,  3.1710e-02,\n",
       "            1.5654e-02,  3.9292e-02, -7.3951e-03,  3.3634e-02,  2.5507e-02,\n",
       "            1.0693e-02,  1.0362e-02,  2.4886e-02,  3.0753e-02,  1.9742e-02,\n",
       "            2.6566e-02,  3.4208e-02,  1.4054e-02,  6.2279e-02, -5.1336e-03,\n",
       "           -3.3956e-03,  1.6031e-02,  2.8677e-02,  1.5798e-02, -3.3445e-03,\n",
       "            1.1143e-02, -6.0977e-03,  1.5932e-02,  2.7663e-02,  2.1324e-02,\n",
       "            2.8826e-02,  3.3788e-02,  2.6629e-02,  1.2251e-02, -2.6365e-02,\n",
       "           -1.7078e-02,  2.7252e-02,  5.3638e-02, -2.5432e-02,  1.1900e-02,\n",
       "           -1.5582e-03, -7.3316e-03,  4.2760e-02, -1.2774e-02,  3.9831e-02,\n",
       "           -1.3043e-03,  3.8751e-02,  3.3525e-02, -1.1954e-02,  3.0281e-02,\n",
       "            1.5010e-02,  4.3106e-02,  2.6567e-02,  2.5265e-02,  2.3171e-02,\n",
       "            5.0346e-03,  1.9416e-02, -5.4689e-03,  2.5772e-02,  2.1729e-02,\n",
       "            2.4199e-02,  3.3944e-02,  2.9970e-02,  5.4499e-02,  1.2004e-02,\n",
       "            3.9265e-02, -1.0019e-02,  1.3086e-02,  1.0103e-02,  3.7902e-02,\n",
       "            2.7688e-02,  3.7119e-02,  4.2533e-03, -1.7836e-02, -6.6811e-03,\n",
       "            1.7498e-02,  8.2833e-03,  1.7799e-02,  2.2877e-02,  4.9634e-02,\n",
       "            1.2511e-02,  1.3354e-02, -1.8062e-02, -2.8347e-02, -2.6685e-02,\n",
       "            4.7702e-02, -8.8419e-06, -2.3876e-02,  2.8839e-02, -1.6787e-02,\n",
       "            7.6084e-03,  5.1140e-03,  6.3063e-03,  2.9464e-02,  3.9283e-02,\n",
       "            1.0363e-02,  2.1709e-02, -6.6520e-03, -2.0796e-03,  3.9445e-02,\n",
       "           -4.9059e-09,  1.0616e-02,  8.0896e-03,  2.1879e-02,  3.5120e-02,\n",
       "            3.0645e-02,  9.3149e-03,  5.4707e-02,  2.3269e-03,  1.5949e-02,\n",
       "            2.4598e-02,  1.4206e-02,  2.7242e-02, -3.4710e-03, -9.3103e-04,\n",
       "            3.0386e-02,  1.7703e-02, -2.3042e-03,  3.6000e-02, -3.0614e-02,\n",
       "            3.2597e-02, -1.4107e-02,  2.9800e-02, -5.8236e-02, -2.1750e-03,\n",
       "            3.8504e-02, -2.3477e-02, -5.9019e-04,  1.8785e-02,  7.0521e-03,\n",
       "            1.0848e-02,  3.6510e-03, -2.4027e-03,  1.7161e-02,  2.3612e-02,\n",
       "            5.2995e-02,  2.0524e-02,  1.5111e-02, -3.0191e-04,  4.3529e-02,\n",
       "            7.2324e-03, -7.0516e-05,  8.9196e-03, -4.1703e-03,  4.7172e-02,\n",
       "           -1.6370e-02,  1.7681e-02,  2.1341e-02, -4.8791e-03, -1.3586e-03,\n",
       "            4.9276e-03, -8.7349e-03,  2.2398e-02,  2.8585e-02, -5.1200e-04,\n",
       "            3.8365e-02,  8.3736e-03, -3.2177e-02, -8.8800e-05,  2.9958e-02,\n",
       "            1.8220e-03,  2.5495e-02,  1.1343e-02,  3.0292e-02,  2.4935e-02,\n",
       "            1.6567e-02,  2.2077e-02, -2.6891e-04,  4.9781e-02, -6.7237e-04,\n",
       "            1.1040e-02,  3.8855e-02, -1.8141e-02,  3.5049e-02, -4.3543e-04,\n",
       "            1.3852e-02,  5.3732e-04,  3.0664e-02,  2.2411e-02,  3.6564e-02,\n",
       "            5.4521e-02,  1.1972e-02,  4.7619e-02,  2.3454e-02,  1.7667e-02,\n",
       "            2.0889e-02,  1.4246e-02,  1.2666e-02, -1.9775e-03,  6.7440e-03,\n",
       "           -3.3093e-02,  4.3598e-02,  1.5315e-02,  2.4657e-02, -2.2537e-02,\n",
       "           -3.1501e-02,  6.2094e-03,  1.5981e-02,  7.9677e-02,  5.7534e-02,\n",
       "            2.4706e-02,  1.3899e-02,  2.8068e-02,  1.9346e-02,  5.3317e-02,\n",
       "            4.1512e-02,  2.6863e-02,  5.2926e-03, -1.0617e-03,  6.9825e-03,\n",
       "            2.0186e-02,  5.3091e-02,  7.5843e-03, -1.3157e-02,  4.5998e-02,\n",
       "            1.4068e-02, -1.9112e-02,  4.8943e-02,  2.7578e-02,  3.1725e-02,\n",
       "            2.0780e-02,  4.7927e-02, -7.6701e-04,  2.2606e-02, -3.0867e-06,\n",
       "            2.3878e-02, -7.7916e-04, -1.8761e-02, -4.5473e-03,  7.3495e-03,\n",
       "           -1.4507e-02,  3.2773e-02,  4.4663e-02,  3.7601e-02,  2.3308e-02,\n",
       "           -3.1608e-02,  2.3318e-02, -1.9957e-02,  5.8038e-02,  1.0391e-02,\n",
       "            2.1266e-02,  2.5539e-02,  1.7227e-02,  3.2036e-02,  2.3718e-02,\n",
       "           -2.8844e-08,  3.2873e-02, -2.4211e-02,  2.7418e-02,  4.0500e-02,\n",
       "           -2.1915e-08,  2.1604e-02,  1.2623e-02,  1.4022e-02,  2.9166e-02,\n",
       "            1.4445e-02,  4.0679e-02,  5.9889e-03,  2.9064e-02, -7.1650e-03,\n",
       "           -2.7302e-03,  1.0903e-02,  1.3187e-02, -7.6374e-04,  9.8285e-03,\n",
       "            5.1421e-03, -4.4485e-03, -3.2946e-02,  2.1548e-02, -1.8839e-02,\n",
       "           -1.5233e-02,  1.0965e-02, -2.7970e-02, -3.0634e-02,  3.9291e-02,\n",
       "           -1.6609e-02,  9.5324e-03,  5.0719e-03,  3.1313e-03,  3.8227e-02,\n",
       "            1.1446e-02,  3.5579e-02,  2.4901e-02,  4.4992e-02,  1.6554e-02,\n",
       "            2.6434e-02,  5.3033e-03,  1.7069e-02,  3.4892e-02, -2.7022e-02,\n",
       "            7.4869e-02,  2.7819e-02,  3.8062e-02,  3.5693e-02,  3.2502e-02,\n",
       "            1.0573e-02,  3.1180e-03,  4.4814e-03,  1.0406e-02, -1.3642e-06,\n",
       "            2.8103e-02,  2.0788e-03, -1.4163e-03, -1.4728e-02,  2.5185e-02,\n",
       "            1.2720e-02,  1.2923e-02,  4.8746e-02,  5.6803e-02,  3.7028e-02,\n",
       "            2.1285e-02,  6.4430e-02,  2.2535e-02,  5.4700e-02,  1.4028e-02,\n",
       "           -1.2804e-02, -2.9018e-02, -1.2719e-05, -1.0225e-03, -3.9901e-13,\n",
       "            1.1636e-02,  4.7106e-03,  3.2101e-02,  2.7193e-02,  1.3010e-02,\n",
       "           -1.8238e-03,  1.0487e-02,  2.9010e-02,  3.1095e-02,  4.5239e-02,\n",
       "            1.6363e-02,  1.3033e-02,  5.5167e-02,  2.2260e-02, -4.6652e-03,\n",
       "            1.7261e-02, -1.0342e-03,  4.4785e-02, -8.4687e-03,  2.5651e-02,\n",
       "            1.3432e-02,  1.2628e-02,  3.9425e-02,  5.2306e-02,  5.5201e-02,\n",
       "            1.0353e-02, -1.1609e-02,  3.2700e-02, -3.1457e-02,  4.3615e-02,\n",
       "            5.3420e-03,  2.6343e-02,  4.9342e-02,  1.2176e-02,  1.4570e-02,\n",
       "            7.3326e-03,  9.8725e-03,  2.2501e-02,  4.0803e-02, -1.0053e-03,\n",
       "            3.4661e-02,  1.1612e-02,  1.2305e-02, -6.4586e-05, -1.2969e-02,\n",
       "            2.7568e-02,  1.0664e-02,  6.9611e-03,  3.4641e-02,  1.3366e-02,\n",
       "            4.5492e-02, -7.3817e-03,  3.8768e-02,  3.7477e-02, -1.7636e-03,\n",
       "            5.3348e-02, -4.4015e-02,  1.9072e-02,  1.6163e-02,  2.6866e-02,\n",
       "            7.5696e-02, -1.0044e-02,  5.5924e-02,  5.3700e-03,  2.9103e-02,\n",
       "            6.1821e-02,  2.7460e-02, -1.7385e-02,  3.0971e-02, -6.5651e-03,\n",
       "           -4.0768e-02,  4.0530e-02,  2.9009e-02,  3.7893e-02,  3.1091e-02,\n",
       "            4.0877e-02, -2.4776e-03,  2.2223e-02,  5.5185e-03,  3.8347e-02,\n",
       "            1.0470e-02,  4.2119e-02, -4.4516e-03, -1.5069e-02,  2.2016e-02,\n",
       "            5.9205e-03, -1.1002e-02,  3.1883e-02,  3.5831e-02,  2.1583e-02,\n",
       "           -3.6121e-05,  2.0044e-02, -4.3332e-02, -1.7694e-02,  1.7214e-02,\n",
       "            3.0059e-02,  2.2235e-02,  4.7614e-02, -1.9172e-02,  9.0174e-03,\n",
       "            3.7083e-02, -9.1027e-03, -1.4730e-02,  5.1330e-02,  1.1484e-02,\n",
       "            4.5048e-02,  4.2471e-02,  2.7057e-02,  2.2518e-02,  2.9487e-02,\n",
       "            3.1719e-02,  6.7779e-02,  5.5568e-02,  2.2471e-02,  2.8837e-02,\n",
       "           -2.4298e-03,  5.4944e-03], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-0.0208,  0.1673, -0.0080,  ..., -0.1305, -0.0343, -0.3798],\n",
       "           [ 0.0330, -0.1718, -0.3595,  ..., -0.0560, -0.0394,  0.0170]],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 0.0288, -0.0288], requires_grad=True)],\n",
       "  'lr': 0.001,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0.001,\n",
       "  'amsgrad': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'capturable': False,\n",
       "  'differentiable': False,\n",
       "  'fused': None}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in optimizer.param_groups:\n",
    "    params[\"lr\"] *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[ 5.0617e-40,  1.3606e-15, -2.2866e-40,  ..., -4.2641e-17,\n",
       "             3.1035e-39, -6.7272e-22],\n",
       "           [-8.5938e-02,  3.2254e-01,  5.0457e-02,  ..., -3.8322e-02,\n",
       "            -8.9960e-02, -4.4433e-03],\n",
       "           [ 1.2289e-01,  6.0114e-02,  1.8644e-01,  ..., -2.9667e-01,\n",
       "            -2.7566e-01, -1.9465e-01],\n",
       "           ...,\n",
       "           [-9.0253e-06,  6.6264e-05, -3.1442e-04,  ...,  1.6112e-04,\n",
       "             3.4517e-05,  6.5777e-07],\n",
       "           [ 6.4758e-40, -2.0265e-12,  6.6061e-41,  ..., -1.3085e-18,\n",
       "            -3.8977e-39, -4.2688e-40],\n",
       "           [ 4.9056e-05, -3.1769e-04, -2.2225e-04,  ...,  1.2772e-04,\n",
       "             1.0306e-04, -6.7761e-05]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[ 4.9720e-03, -3.8873e-03,  2.8663e-03,  ..., -3.0103e-04,\n",
       "            -5.1750e-04,  5.6413e-03],\n",
       "           [ 1.6180e-03, -5.2169e-02, -1.9099e-02,  ...,  1.8352e-02,\n",
       "            -3.8149e-02, -1.2734e-02],\n",
       "           [ 4.3576e-03, -6.9534e-02, -2.8206e-02,  ...,  2.3572e-02,\n",
       "            -3.8157e-02, -4.8085e-04],\n",
       "           ...,\n",
       "           [ 3.8063e-03,  1.0694e-02,  7.3188e-03,  ..., -8.6752e-03,\n",
       "             9.0609e-03, -6.3648e-03],\n",
       "           [-2.1297e-29,  2.8666e-33,  6.8726e-17,  ..., -3.6290e-19,\n",
       "             9.4901e-35, -2.2122e-16],\n",
       "           [-4.1916e-03,  6.0640e-02,  1.5416e-02,  ..., -2.0623e-02,\n",
       "             4.6257e-02,  1.5298e-02]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-4.8541e-02,  2.5032e-02,  3.0954e-02,  7.3671e-03,  4.6973e-02,\n",
       "            4.6491e-02, -2.4345e-02,  2.4159e-02,  1.1587e-02,  4.0508e-02,\n",
       "           -7.5365e-09,  1.6322e-02,  5.6569e-02, -1.4966e-02, -2.7507e-03,\n",
       "           -1.4014e-02,  4.2039e-02,  2.8920e-02,  1.5983e-02,  4.2507e-03,\n",
       "            1.5304e-02,  6.9926e-02,  4.3077e-02,  3.6175e-03,  2.5455e-02,\n",
       "           -3.6211e-02, -6.3910e-04,  4.2066e-03,  1.1534e-02,  3.2473e-02,\n",
       "            5.7871e-02,  5.2768e-02,  2.0664e-02, -1.9505e-02,  3.6803e-02,\n",
       "            6.0036e-02, -1.9276e-02,  3.8523e-02, -8.9749e-03,  2.9042e-02,\n",
       "            5.4590e-03, -1.5635e-02, -4.5276e-03, -4.4159e-04,  2.7347e-03,\n",
       "            1.1346e-02, -5.3011e-04,  1.8815e-02, -2.0194e-03,  5.5595e-02,\n",
       "           -3.3343e-06,  2.6839e-02,  1.2669e-02, -4.6802e-03,  3.1302e-02,\n",
       "            2.3669e-02, -1.0479e-02,  3.6923e-02, -1.5955e-02, -1.8176e-03,\n",
       "            3.5309e-02,  3.3806e-02, -2.4309e-02,  2.9455e-02, -1.3858e-02,\n",
       "            6.4852e-03,  4.1651e-02,  3.5122e-02,  2.1591e-02,  6.8456e-03,\n",
       "           -7.0643e-03,  2.4218e-02,  3.4863e-02, -1.5273e-02, -8.8954e-03,\n",
       "           -8.7964e-03,  1.0745e-02,  3.9358e-02, -2.1034e-06,  3.1710e-02,\n",
       "            1.5654e-02,  3.9292e-02, -7.3951e-03,  3.3634e-02,  2.5507e-02,\n",
       "            1.0693e-02,  1.0362e-02,  2.4886e-02,  3.0753e-02,  1.9742e-02,\n",
       "            2.6566e-02,  3.4208e-02,  1.4054e-02,  6.2279e-02, -5.1336e-03,\n",
       "           -3.3956e-03,  1.6031e-02,  2.8677e-02,  1.5798e-02, -3.3445e-03,\n",
       "            1.1143e-02, -6.0977e-03,  1.5932e-02,  2.7663e-02,  2.1324e-02,\n",
       "            2.8826e-02,  3.3788e-02,  2.6629e-02,  1.2251e-02, -2.6365e-02,\n",
       "           -1.7078e-02,  2.7252e-02,  5.3638e-02, -2.5432e-02,  1.1900e-02,\n",
       "           -1.5582e-03, -7.3316e-03,  4.2760e-02, -1.2774e-02,  3.9831e-02,\n",
       "           -1.3043e-03,  3.8751e-02,  3.3525e-02, -1.1954e-02,  3.0281e-02,\n",
       "            1.5010e-02,  4.3106e-02,  2.6567e-02,  2.5265e-02,  2.3171e-02,\n",
       "            5.0346e-03,  1.9416e-02, -5.4689e-03,  2.5772e-02,  2.1729e-02,\n",
       "            2.4199e-02,  3.3944e-02,  2.9970e-02,  5.4499e-02,  1.2004e-02,\n",
       "            3.9265e-02, -1.0019e-02,  1.3086e-02,  1.0103e-02,  3.7902e-02,\n",
       "            2.7688e-02,  3.7119e-02,  4.2533e-03, -1.7836e-02, -6.6811e-03,\n",
       "            1.7498e-02,  8.2833e-03,  1.7799e-02,  2.2877e-02,  4.9634e-02,\n",
       "            1.2511e-02,  1.3354e-02, -1.8062e-02, -2.8347e-02, -2.6685e-02,\n",
       "            4.7702e-02, -8.8419e-06, -2.3876e-02,  2.8839e-02, -1.6787e-02,\n",
       "            7.6084e-03,  5.1140e-03,  6.3063e-03,  2.9464e-02,  3.9283e-02,\n",
       "            1.0363e-02,  2.1709e-02, -6.6520e-03, -2.0796e-03,  3.9445e-02,\n",
       "           -4.9059e-09,  1.0616e-02,  8.0896e-03,  2.1879e-02,  3.5120e-02,\n",
       "            3.0645e-02,  9.3149e-03,  5.4707e-02,  2.3269e-03,  1.5949e-02,\n",
       "            2.4598e-02,  1.4206e-02,  2.7242e-02, -3.4710e-03, -9.3103e-04,\n",
       "            3.0386e-02,  1.7703e-02, -2.3042e-03,  3.6000e-02, -3.0614e-02,\n",
       "            3.2597e-02, -1.4107e-02,  2.9800e-02, -5.8236e-02, -2.1750e-03,\n",
       "            3.8504e-02, -2.3477e-02, -5.9019e-04,  1.8785e-02,  7.0521e-03,\n",
       "            1.0848e-02,  3.6510e-03, -2.4027e-03,  1.7161e-02,  2.3612e-02,\n",
       "            5.2995e-02,  2.0524e-02,  1.5111e-02, -3.0191e-04,  4.3529e-02,\n",
       "            7.2324e-03, -7.0516e-05,  8.9196e-03, -4.1703e-03,  4.7172e-02,\n",
       "           -1.6370e-02,  1.7681e-02,  2.1341e-02, -4.8791e-03, -1.3586e-03,\n",
       "            4.9276e-03, -8.7349e-03,  2.2398e-02,  2.8585e-02, -5.1200e-04,\n",
       "            3.8365e-02,  8.3736e-03, -3.2177e-02, -8.8800e-05,  2.9958e-02,\n",
       "            1.8220e-03,  2.5495e-02,  1.1343e-02,  3.0292e-02,  2.4935e-02,\n",
       "            1.6567e-02,  2.2077e-02, -2.6891e-04,  4.9781e-02, -6.7237e-04,\n",
       "            1.1040e-02,  3.8855e-02, -1.8141e-02,  3.5049e-02, -4.3543e-04,\n",
       "            1.3852e-02,  5.3732e-04,  3.0664e-02,  2.2411e-02,  3.6564e-02,\n",
       "            5.4521e-02,  1.1972e-02,  4.7619e-02,  2.3454e-02,  1.7667e-02,\n",
       "            2.0889e-02,  1.4246e-02,  1.2666e-02, -1.9775e-03,  6.7440e-03,\n",
       "           -3.3093e-02,  4.3598e-02,  1.5315e-02,  2.4657e-02, -2.2537e-02,\n",
       "           -3.1501e-02,  6.2094e-03,  1.5981e-02,  7.9677e-02,  5.7534e-02,\n",
       "            2.4706e-02,  1.3899e-02,  2.8068e-02,  1.9346e-02,  5.3317e-02,\n",
       "            4.1512e-02,  2.6863e-02,  5.2926e-03, -1.0617e-03,  6.9825e-03,\n",
       "            2.0186e-02,  5.3091e-02,  7.5843e-03, -1.3157e-02,  4.5998e-02,\n",
       "            1.4068e-02, -1.9112e-02,  4.8943e-02,  2.7578e-02,  3.1725e-02,\n",
       "            2.0780e-02,  4.7927e-02, -7.6701e-04,  2.2606e-02, -3.0867e-06,\n",
       "            2.3878e-02, -7.7916e-04, -1.8761e-02, -4.5473e-03,  7.3495e-03,\n",
       "           -1.4507e-02,  3.2773e-02,  4.4663e-02,  3.7601e-02,  2.3308e-02,\n",
       "           -3.1608e-02,  2.3318e-02, -1.9957e-02,  5.8038e-02,  1.0391e-02,\n",
       "            2.1266e-02,  2.5539e-02,  1.7227e-02,  3.2036e-02,  2.3718e-02,\n",
       "           -2.8844e-08,  3.2873e-02, -2.4211e-02,  2.7418e-02,  4.0500e-02,\n",
       "           -2.1915e-08,  2.1604e-02,  1.2623e-02,  1.4022e-02,  2.9166e-02,\n",
       "            1.4445e-02,  4.0679e-02,  5.9889e-03,  2.9064e-02, -7.1650e-03,\n",
       "           -2.7302e-03,  1.0903e-02,  1.3187e-02, -7.6374e-04,  9.8285e-03,\n",
       "            5.1421e-03, -4.4485e-03, -3.2946e-02,  2.1548e-02, -1.8839e-02,\n",
       "           -1.5233e-02,  1.0965e-02, -2.7970e-02, -3.0634e-02,  3.9291e-02,\n",
       "           -1.6609e-02,  9.5324e-03,  5.0719e-03,  3.1313e-03,  3.8227e-02,\n",
       "            1.1446e-02,  3.5579e-02,  2.4901e-02,  4.4992e-02,  1.6554e-02,\n",
       "            2.6434e-02,  5.3033e-03,  1.7069e-02,  3.4892e-02, -2.7022e-02,\n",
       "            7.4869e-02,  2.7819e-02,  3.8062e-02,  3.5693e-02,  3.2502e-02,\n",
       "            1.0573e-02,  3.1180e-03,  4.4814e-03,  1.0406e-02, -1.3642e-06,\n",
       "            2.8103e-02,  2.0788e-03, -1.4163e-03, -1.4728e-02,  2.5185e-02,\n",
       "            1.2720e-02,  1.2923e-02,  4.8746e-02,  5.6803e-02,  3.7028e-02,\n",
       "            2.1285e-02,  6.4430e-02,  2.2535e-02,  5.4700e-02,  1.4028e-02,\n",
       "           -1.2804e-02, -2.9018e-02, -1.2719e-05, -1.0225e-03, -3.9901e-13,\n",
       "            1.1636e-02,  4.7106e-03,  3.2101e-02,  2.7193e-02,  1.3010e-02,\n",
       "           -1.8238e-03,  1.0487e-02,  2.9010e-02,  3.1095e-02,  4.5239e-02,\n",
       "            1.6363e-02,  1.3033e-02,  5.5167e-02,  2.2260e-02, -4.6652e-03,\n",
       "            1.7261e-02, -1.0342e-03,  4.4785e-02, -8.4687e-03,  2.5651e-02,\n",
       "            1.3432e-02,  1.2628e-02,  3.9425e-02,  5.2306e-02,  5.5201e-02,\n",
       "            1.0353e-02, -1.1609e-02,  3.2700e-02, -3.1457e-02,  4.3615e-02,\n",
       "            5.3420e-03,  2.6343e-02,  4.9342e-02,  1.2176e-02,  1.4570e-02,\n",
       "            7.3326e-03,  9.8725e-03,  2.2501e-02,  4.0803e-02, -1.0053e-03,\n",
       "            3.4661e-02,  1.1612e-02,  1.2305e-02, -6.4586e-05, -1.2969e-02,\n",
       "            2.7568e-02,  1.0664e-02,  6.9611e-03,  3.4641e-02,  1.3366e-02,\n",
       "            4.5492e-02, -7.3817e-03,  3.8768e-02,  3.7477e-02, -1.7636e-03,\n",
       "            5.3348e-02, -4.4015e-02,  1.9072e-02,  1.6163e-02,  2.6866e-02,\n",
       "            7.5696e-02, -1.0044e-02,  5.5924e-02,  5.3700e-03,  2.9103e-02,\n",
       "            6.1821e-02,  2.7460e-02, -1.7385e-02,  3.0971e-02, -6.5651e-03,\n",
       "           -4.0768e-02,  4.0530e-02,  2.9009e-02,  3.7893e-02,  3.1091e-02,\n",
       "            4.0877e-02, -2.4776e-03,  2.2223e-02,  5.5185e-03,  3.8347e-02,\n",
       "            1.0470e-02,  4.2119e-02, -4.4516e-03, -1.5069e-02,  2.2016e-02,\n",
       "            5.9205e-03, -1.1002e-02,  3.1883e-02,  3.5831e-02,  2.1583e-02,\n",
       "           -3.6121e-05,  2.0044e-02, -4.3332e-02, -1.7694e-02,  1.7214e-02,\n",
       "            3.0059e-02,  2.2235e-02,  4.7614e-02, -1.9172e-02,  9.0174e-03,\n",
       "            3.7083e-02, -9.1027e-03, -1.4730e-02,  5.1330e-02,  1.1484e-02,\n",
       "            4.5048e-02,  4.2471e-02,  2.7057e-02,  2.2518e-02,  2.9487e-02,\n",
       "            3.1719e-02,  6.7779e-02,  5.5568e-02,  2.2471e-02,  2.8837e-02,\n",
       "           -2.4298e-03,  5.4944e-03], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-0.0208,  0.1673, -0.0080,  ..., -0.1305, -0.0343, -0.3798],\n",
       "           [ 0.0330, -0.1718, -0.3595,  ..., -0.0560, -0.0394,  0.0170]],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 0.0288, -0.0288], requires_grad=True)],\n",
       "  'lr': 0.0001,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0.001,\n",
       "  'amsgrad': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'capturable': False,\n",
       "  'differentiable': False,\n",
       "  'fused': None}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (y, X, offsets) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X, offsets)\n",
    "\n",
    "        # 2. Calculating and accumulating loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        l2_lambda = 0.001\n",
    "        l2_regularization = torch.tensor(0.0, requires_grad=True)\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"bias\" not in name:\n",
    "                l2_regularization = l2_regularization + torch.norm(param, p=2)\n",
    "\n",
    "        loss = loss + l2_lambda * l2_regularization\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
    "        # print(y_pred, y_pred_class, train_prec, \"\\n\")\n",
    "        # break\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def test_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (y, X, offsets) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X, offsets)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_regularization = torch.tensor(0.0, requires_grad=True)\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"bias\" not in name:\n",
    "                    l2_regularization = l2_regularization + torch.norm(param, p=2)\n",
    "            loss = loss + l2_lambda * l2_regularization\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += (test_pred_labels == y).sum().item() / len(test_pred_labels)\n",
    "\n",
    "            # break\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    "):\n",
    "    # Create empty results dictionary\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "    }\n",
    "\n",
    "    # Make sure model on target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "        )\n",
    "        test_loss, test_acc = test_step(\n",
    "            model=model, dataloader=test_dataloader, loss_fn=loss_fn, device=device\n",
    "        )\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0]),\n",
       " tensor([ 130,   51,   73,  ...,   13, 1053,   36]),\n",
       " tensor([   0,  328,  567,  819, 1168, 1623, 1848, 2067, 2396, 2469, 2712, 3086,\n",
       "         5497, 5501, 5829, 5872]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24d3f985616439fbe9a7a9e3b6652cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9677 | train_acc: 0.6699 | test_loss: 0.8158 | test_acc: 0.7250\n",
      "Epoch: 2 | train_loss: 0.7218 | train_acc: 0.7746 | test_loss: 0.6284 | test_acc: 0.8000\n",
      "Epoch: 3 | train_loss: 0.5261 | train_acc: 0.8521 | test_loss: 0.5107 | test_acc: 0.8385\n",
      "Epoch: 4 | train_loss: 0.4067 | train_acc: 0.8856 | test_loss: 0.4578 | test_acc: 0.8472\n",
      "Epoch: 5 | train_loss: 0.3175 | train_acc: 0.9217 | test_loss: 0.4226 | test_acc: 0.8347\n",
      "Epoch: 6 | train_loss: 0.2737 | train_acc: 0.9252 | test_loss: 0.4344 | test_acc: 0.8503\n",
      "Epoch: 7 | train_loss: 0.2363 | train_acc: 0.9423 | test_loss: 0.4124 | test_acc: 0.8438\n",
      "Epoch: 8 | train_loss: 0.2158 | train_acc: 0.9476 | test_loss: 0.4307 | test_acc: 0.8500\n",
      "Epoch: 9 | train_loss: 0.1961 | train_acc: 0.9551 | test_loss: 0.4146 | test_acc: 0.8507\n",
      "Epoch: 10 | train_loss: 0.1829 | train_acc: 0.9608 | test_loss: 0.4236 | test_acc: 0.8434\n",
      "Epoch: 11 | train_loss: 0.1760 | train_acc: 0.9595 | test_loss: 0.4263 | test_acc: 0.8420\n",
      "Epoch: 12 | train_loss: 0.1624 | train_acc: 0.9648 | test_loss: 0.4308 | test_acc: 0.8378\n",
      "Epoch: 13 | train_loss: 0.1563 | train_acc: 0.9692 | test_loss: 0.4335 | test_acc: 0.8399\n",
      "Epoch: 14 | train_loss: 0.1535 | train_acc: 0.9688 | test_loss: 0.4529 | test_acc: 0.8326\n",
      "Epoch: 15 | train_loss: 0.1490 | train_acc: 0.9723 | test_loss: 0.4192 | test_acc: 0.8559\n",
      "Epoch: 16 | train_loss: 0.1422 | train_acc: 0.9745 | test_loss: 0.4323 | test_acc: 0.8507\n",
      "Epoch: 17 | train_loss: 0.1409 | train_acc: 0.9718 | test_loss: 0.4255 | test_acc: 0.8472\n",
      "Epoch: 18 | train_loss: 0.1332 | train_acc: 0.9745 | test_loss: 0.4271 | test_acc: 0.8490\n",
      "Epoch: 19 | train_loss: 0.1332 | train_acc: 0.9736 | test_loss: 0.4251 | test_acc: 0.8490\n",
      "Epoch: 20 | train_loss: 0.1312 | train_acc: 0.9776 | test_loss: 0.4395 | test_acc: 0.8399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.9676783890791343,\n",
       "  0.7217654415839155,\n",
       "  0.5261246675336865,\n",
       "  0.4067236065444812,\n",
       "  0.31747695845617374,\n",
       "  0.27368229207858236,\n",
       "  0.236286936563925,\n",
       "  0.21580671445584634,\n",
       "  0.19610047057061128,\n",
       "  0.18292804183044903,\n",
       "  0.17596764253898406,\n",
       "  0.16238376936337479,\n",
       "  0.15628138766951963,\n",
       "  0.15347192315778263,\n",
       "  0.1489973974899507,\n",
       "  0.14218394961995137,\n",
       "  0.14088348419943325,\n",
       "  0.13323462298008756,\n",
       "  0.13320934601967604,\n",
       "  0.1312231827842098],\n",
       " 'train_acc': [0.6698943661971831,\n",
       "  0.7746478873239436,\n",
       "  0.852112676056338,\n",
       "  0.8855633802816901,\n",
       "  0.9216549295774648,\n",
       "  0.9251760563380281,\n",
       "  0.9423415492957746,\n",
       "  0.9476232394366197,\n",
       "  0.9551056338028169,\n",
       "  0.9608274647887324,\n",
       "  0.9595070422535211,\n",
       "  0.9647887323943662,\n",
       "  0.9691901408450704,\n",
       "  0.96875,\n",
       "  0.9722711267605634,\n",
       "  0.9744718309859155,\n",
       "  0.971830985915493,\n",
       "  0.9744718309859155,\n",
       "  0.9735915492957746,\n",
       "  0.9775528169014085],\n",
       " 'test_loss': [0.8157598922650019,\n",
       "  0.6283977934055858,\n",
       "  0.5106584520803558,\n",
       "  0.4577510141664081,\n",
       "  0.4226198610332277,\n",
       "  0.4344066455960274,\n",
       "  0.4123704706629117,\n",
       "  0.430661841813061,\n",
       "  0.41462839146455127,\n",
       "  0.4235605684419473,\n",
       "  0.4262968707415793,\n",
       "  0.4308124664756987,\n",
       "  0.4334942379759418,\n",
       "  0.4529037939177619,\n",
       "  0.4192404401385122,\n",
       "  0.43233143765893245,\n",
       "  0.4254896945009629,\n",
       "  0.4271376083294551,\n",
       "  0.42514046788629556,\n",
       "  0.4395320556230015],\n",
       " 'test_acc': [0.7250000000000001,\n",
       "  0.8,\n",
       "  0.8385416666666666,\n",
       "  0.8472222222222222,\n",
       "  0.8347222222222223,\n",
       "  0.8503472222222223,\n",
       "  0.84375,\n",
       "  0.8500000000000001,\n",
       "  0.8506944444444444,\n",
       "  0.8434027777777778,\n",
       "  0.8420138888888888,\n",
       "  0.8378472222222223,\n",
       "  0.8399305555555556,\n",
       "  0.8326388888888889,\n",
       "  0.8559027777777778,\n",
       "  0.8506944444444444,\n",
       "  0.8472222222222222,\n",
       "  0.8489583333333334,\n",
       "  0.8489583333333334,\n",
       "  0.8399305555555556]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_dataloader, test_dataloader, optimizer, criterion, EPOCHS, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import F1Score\n",
    "\n",
    "f1_fn = F1Score(task=\"binary\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchmetrics.classification.f_beta.BinaryF1Score"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f1_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand((16,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X,offset=collate_batch([[\"The sun rises in the east and sets in the west.\",0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X,offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.eval()\n",
    "\n",
    "# Setup test loss and test accuracy values\n",
    "test_loss, test_acc = 0, 0\n",
    "\n",
    "# Turn on inference context manager\n",
    "with torch.inference_mode():\n",
    "    \n",
    "    # Send data to target device\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    # 1. Forward pass\n",
    "    test_pred_logits = new_model(X, offset)\n",
    "\n",
    "    # 2. Calculate and accumulate loss\n",
    "    # loss = loss_fn(test_pred_logits, y)\n",
    "    # test_loss += loss.item()\n",
    "\n",
    "    # Calculate and accumulate accuracy\n",
    "    test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "    # test_acc += (test_pred_labels == y).sum().item() / len(test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels,test_pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# def save_model(model: torch.nn.Module, target_dir: str, model_name: str):\n",
    "#     # Create target directory\n",
    "#     target_dir_path = Path(target_dir)\n",
    "#     target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # Create model save path\n",
    "#     model_save_path = target_dir_path / model_name\n",
    "\n",
    "#     # Save the model state_dict()\n",
    "#     print(f\" Saving model to: {model_save_path}\")\n",
    "#     torch.save(obj=model.state_dict(), f=model_save_path)\n",
    "    \n",
    "# save_model(\n",
    "#     model=model, target_dir=\"models\", model_name=f\"first_model.pth\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = TextClassificationModel(vocab_size, embed_size, num_class).to(device)\n",
    "new_model.load_state_dict(torch.load(\"models/first_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = pickle.load(open(\"data.pkl\", \"rb\"))\n",
    "rules = open(\"samples.txt\", \"r\").readlines()\n",
    "rules = [i[0:-1] for i in rules]\n",
    "exemplars=pickle.load(open(\"samples.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text,label in data[:16]:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_setup import get_dataloaders\n",
    "\n",
    "tr_dl, ts_dl, vl = get_dataloaders(\"data.pkl\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=next(iter(tr_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in zip(rules,exemplars):\n",
    "#     print(i,\" \",j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "y = []\n",
    "\n",
    "for text, label in data[:]:\n",
    "    temp = []\n",
    "    for i in range(len(rules)):\n",
    "        compiled_pattern = re.compile(rules[i], re.IGNORECASE)\n",
    "        if bool(compiled_pattern.search(text)) and label == 0:\n",
    "            temp.append(0)\n",
    "        else:\n",
    "            temp.append(1)\n",
    "    y.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[0]),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[i for i,j in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_data = [[data[i],y[i]] for i in range(2822)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rule_data),len(rule_data[0]),len(rule_data[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rule_data,open(\"rule_data.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pickle.load(open(\"rule_data.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a),len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a:\n",
    "    print(i[0],i[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[preprocess(i[0]), i[1]] for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"#[A-Za-z0-9]*\", \"\", text)\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[\\n]\", \" \", text)\n",
    "    text = re.sub(r\"['\\\",*%$#()]\", \" \", text)\n",
    "    text = re.sub(r\"[.]\", \" . \", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[preprocess(i[0]), i[1]] for i in rule_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data),len(data[0]),len(data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_setup import get_dataloaders\n",
    "\n",
    "train_dl,test_dl,vl=get_dataloaders(\"rule_data.pkl\",16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, o = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4460/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b,d in enumerate(train_dl):\n",
    "    print(b,d,d[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0].shape,d[1].shape,d[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "y=torch.rand((16,43))\n",
    "y_pred=torch.rand((16,43))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=y-y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for i in temp.reshape(-1):\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "num_classes = 5\n",
    "for i in range(num_classes):\n",
    "    if (y[i] - y_pred[i]).abs() < 0.5:\n",
    "        c += 1\n",
    "acc = c / num_classes\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim.adam\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_nodes, num_class, embedding_matrix):\n",
    "        super(ClassificationNetwork, self).__init__()\n",
    "\n",
    "        # self.glove_embedding = nn.EmbeddingBag.from_pretrained(\n",
    "        #     embedding_matrix, freeze=True\n",
    "        # )\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.dense_block = nn.Sequential(\n",
    "            nn.Linear(embed_dim, num_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_nodes, num_class),\n",
    "        )\n",
    "\n",
    "        self.embedding.apply(self.init_weights)\n",
    "        self.dense_block.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, layer):\n",
    "        initrange = 0.5\n",
    "\n",
    "        if isinstance(layer, nn.Linear or nn.EmbeddingBag):\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # combined_embedding = torch.cat(\n",
    "        #     [self.embedding(text, offsets), self.glove_embedding(text, offsets)],\n",
    "        #     dim=-1,\n",
    "        # )\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.dense_block(embedded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationNetwork(500, 20, 50, 2, None)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"max\", patience=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.lr_scheduler.ReduceLROnPlateau"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "res1=pickle.load(open(\"results\\classification_model_dropout0.6_2508_results.pkl\",\"rb\"))\n",
    "res2=pickle.load(open(\"results\\classification_model_dropout0.5_2508_results.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    train_loss  train_f1  train_acc  test_loss   test_f1  test_acc\n",
       " 17    0.037894  0.989567   0.990423   0.343799  0.887422  0.897327\n",
       " 11    0.112145  0.957060   0.962558   0.311856  0.884228  0.892610\n",
       " 13    0.082809  0.968599   0.970766   0.325911  0.881725  0.896148\n",
       " 18    0.033232  0.988797   0.990423   0.355096  0.880266  0.887893\n",
       " 12    0.086164  0.967595   0.969758   0.316964  0.876140  0.893789\n",
       " 10    0.135249  0.948615   0.953125   0.319740  0.875109  0.883019\n",
       " 8     0.173269  0.921297   0.927275   0.317038  0.871735  0.883019\n",
       " 14    0.073092  0.970906   0.973286   0.338545  0.867581  0.890252\n",
       " 19    0.036350  0.991695   0.992440   0.374309  0.865950  0.878302\n",
       " 16    0.049480  0.980592   0.984375   0.349396  0.865330  0.887972\n",
       " 9     0.158399  0.932475   0.941028   0.306827  0.863489  0.877123\n",
       " 15    0.055742  0.982230   0.981351   0.349301  0.862266  0.893789\n",
       " 7     0.225380  0.899866   0.909634   0.325797  0.853238  0.879481\n",
       " 6     0.248921  0.887587   0.900562   0.334898  0.848641  0.878223\n",
       " 5     0.301629  0.850313   0.868664   0.364640  0.843121  0.869104\n",
       " 3     0.461390  0.807380   0.825965   0.378742  0.837272  0.855818\n",
       " 2     0.628166  0.761654   0.786146   0.399328  0.829762  0.846462\n",
       " 4     0.372067  0.844675   0.859231   0.397414  0.821988  0.826336\n",
       " 1     1.000743  0.709126   0.736103   0.446833  0.817163  0.827594\n",
       " 0     1.836522  0.597117   0.617800   0.540518  0.767699  0.804167,\n",
       "     train_loss  train_f1  train_acc  test_loss   test_f1  test_acc\n",
       " 17    0.021012  0.995407   0.995464   0.348890  0.889166  0.896148\n",
       " 18    0.018294  0.996204   0.996976   0.353329  0.884648  0.893789\n",
       " 14    0.043402  0.990762   0.990927   0.326724  0.884568  0.899686\n",
       " 11    0.077190  0.971128   0.975662   0.319784  0.882826  0.894969\n",
       " 13    0.054349  0.981115   0.982359   0.333857  0.882032  0.894969\n",
       " 10    0.104139  0.968988   0.970622   0.325956  0.878440  0.885456\n",
       " 12    0.057669  0.977075   0.979839   0.326633  0.876999  0.892610\n",
       " 16    0.028396  0.991941   0.992944   0.342463  0.875254  0.896226\n",
       " 9     0.115336  0.953606   0.959029   0.317399  0.872904  0.884277\n",
       " 8     0.136866  0.947339   0.950965   0.328593  0.872575  0.887657\n",
       " 15    0.033773  0.987413   0.988911   0.353482  0.864737  0.897406\n",
       " 6     0.216389  0.911331   0.920867   0.327567  0.860898  0.885377\n",
       " 19    0.024659  0.993424   0.993952   0.423550  0.858055  0.865330\n",
       " 3     0.374995  0.828822   0.847134   0.358684  0.855669  0.866431\n",
       " 7     0.178815  0.926903   0.932676   0.324650  0.852881  0.873506\n",
       " 4     0.297067  0.873524   0.884937   0.354051  0.845804  0.856997\n",
       " 5     0.240578  0.883933   0.898546   0.366690  0.843736  0.869025\n",
       " 2     0.538275  0.786721   0.805660   0.383802  0.828774  0.845362\n",
       " 1     0.830130  0.724296   0.746328   0.429112  0.814417  0.828695\n",
       " 0     1.588085  0.599325   0.621328   0.500199  0.777446  0.808884)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res1).sort_values(by=\"test_f1\",ascending=False),pd.DataFrame(res2).sort_values(by=\"test_f1\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
